# 机器学习指标

> 本文参考： 
>
> 1. [一文看懂机器学习指标：准确率、精准率、召回率、`F1`、`ROC`曲线、`AUC`曲线]([一文看懂机器学习指标：准确率、精准率、召回率、F1、ROC曲线、AUC曲线 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/93107394))
> 2. [一文让你彻底理解准确率、精确率、召回率、真正率、假正率、`ROC/AUC`]([一文让你彻底理解准确率，精准率，召回率，真正率，假正率，ROC/AUC - AIQ (6aiq.com)](https://www.6aiq.com/article/1549986548173))



## 1.机器学习评估指标概述

> 本文讲解**分类问题**的混淆矩阵和各种评估指标的计算公式——
>
> 准确率、精确率、召回率、`F1`、`ROC`曲线、`AUC`曲线

![img](https://pic1.zhimg.com/80/v2-ae83f00194575ce8af26613c6ccc6868_720w.webp)

- **分类问题** 评估指标：
  - 准确率——Accuracy
  - 精确率（差准率）——Precision
  - 召回率（查全率）——Recall
  - `F1`分数
  - `ROC`曲线
  - `AUC`曲线
- **回归问题**评估指标：
  - `MAE`
  - `MSE`



## 2.分类问题图解

下面用具体的例子将分类问题进行图解，帮助大家快速理解分类中出现的各种情况。

举个例子：我们有10张照片，5张男性、5张女性，如下图：

![img](https://pic3.zhimg.com/80/v2-8d0aec0b4e4dfb89fe9fe6c0c69c5bce_720w.webp)

有一个**判断性别**的机器学习模型，当我们使用它来判断【是否为男性】时，会出现4种情况，如下图：

![img](https://pic2.zhimg.com/80/v2-c62cfe7dfbc17c229e16948860157911_720w.webp)

1. 实际为男性，且判断为男性（正确）
2. 实际为男性，但判断为女性（错误）
3. 实际为女性，且判断为女性（正确）
4. 实际为女性，但判断为男性（错误）

这4种情况构成了经典的混淆矩阵，如下图：

![img](https://pic4.zhimg.com/80/v2-3b8b884a669e9d650acce6786fd37517_720w.webp)

- `TP`：True Positive，实际为男性，且判断为男性（正确）
- `FN`：False Negative，实际为男性，但判断为女性（错误）
- `TN`：True Negative，实际为女性，且判断为女性（正确）
- `FP`：False Positive，实际为女性，但判断为男性（错误）

> 理解：后面为预测结果，前面是预测结果的正确性
>
> `T P`：预测为`P`（正例），预测对了，本来是正样本，检测为正样本（真阳性）；
>
> `T N`：预测为`N`（负例），预测对了，本来是负样本，检测为负样本（真阴性）；
>
> `F P`：预测为`P`（正例），预测错了，本来是负样本，检测为正样本（假阳性）；
>
> `F N`：预测为`N`（负例），预测错了，本来是正样本，检测为负样本（假阴性）；
>
> `TP+FP+TN+FN`：样本总数
>
> `TP+FN`：实际正样本数
>
> `TP+FP`：预测结果为正样本的总数，包括预测正确的和错误的
>
> `FP+TN`：实际负样本数
>
> `TN+FN`：预测结果为负样本的总数，包括预测正确的和错误的
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210318200644964.png)



## 3.分类评估指标详解

### 3.1 准确率——Accuracy

预测正确的结果占总样本的百分比，公式如下：
$$
Accuracy=\frac{TP+TN}{TP+TN+FP+FN}
$$
![img](https://pic3.zhimg.com/80/v2-72467471528253b65eb6982517ee16b6_720w.webp)

> 虽然准确率可以判断总的正确率，但是在**样本不平衡**的情况下，并不能作为很好指标来衡量结果。
>
> 举个简单的例子：比如在一个总样本中，正样本占90%，负样本占10%，样本是严重不平衡的。对于这种情况，我们只需要将全部样本预测为正样本即可得到90%的高准确率，但实际上我们并没有很用心的分类，只是随便无脑一分。
>
> 这就说明了：**由于样本不平衡的问题，导致了得到的高准确率结果含有很大的水分，即如果样本不平衡，准确率就会失效**。



> **准确率**的缺点：准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比如，当负样本占 $99\%$ 时，分类器把所有样本都预测为负样本也可以获得 $99\%$ 的准确率。所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。



### 3.2 精确率（差准率）——Precision

**精确率**（Precision）又叫做**查准率**，它是 **针对预测结果** 而言的，它的含义是**在所有被预测为正的样本中实际为正的样本的概率** ，意思就是在预测为正样本的结果中，我们有多少把握可以预测正确，其公式如下：
$$
Precision = \frac{TP}{TP+FP} \\\\
Precision = \frac{True \space Positive}{Predict \space Positive} \\
Predice \space Positive = TP + FP
$$
<img src="https://pic3.zhimg.com/80/v2-362c3497dcada9906816d792048eb58e_720w.webp" alt="img" style="zoom:50%;" />

> 精确率和准确率看上去有些类似，但是完全不同的概念。
>
> **精确率**代表对预测正样本结果中的预测准确准确程度；
>
> 而**准确率**代表整体的预测准确程度；



### 3.2 召回率（查全率）——Recall

#### 3.2.1 召回率

**召回率**（Recall）又叫做查全率，它是 **针对原样本** 而言的，它的含义是 **在实际为正的样本中被预测为正样本的概率**，其公式如下：
$$
Recall = \frac{TP}{TP+FN} \\\\
Recall = \frac{True \space Positive}{Actual \space Positive} \\
Actual \space Positive = TP + FN
$$
![img](https://pic4.zhimg.com/80/v2-b694035a03948c85989f46b38eac848f_720w.webp)

> **召回率的应用场景**： 比如拿网贷违约率为例，相对好用户，我们更关心坏用户，不能错放过任何一个坏用户。因为如果我们过多的将坏用户当成好用户，这样后续可能发生的违约金额会远超过好用户偿还的借贷利息金额，造成严重偿失。**召回率越高，代表实际坏用户被预测出来的概率越高，它的含义类似：宁可错杀一千，绝不放过一个。**



#### 3.2.2 精确率和召回率之间的权衡

精确度和召回率之间存在一种权衡关系是因为它们都是基于分类器的预测结果和实际标签进行计算的，并且受到了分类器决策阈值的影响。

让我们更详细地解释这种权衡关系：

1. **精确度**：精确度是指分类器预测为正类的样本中，真正为正类的比例。其计算公式为：

   1. $$
      精确度 = \frac{TP}{(TP + FP)}
      $$

   2. 其中 `TP` 表示真正的正类样本，`FP` 表示被错误地预测为正类的负类样本。精确度衡量了分类器在所有预测为正类的样本中的准确性。

2. **召回率**：召回率是指分类器成功预测为正类的样本占所有真正正类样本的比例。其计算公式为：

   1. $$
      召回率 = \frac{TP}{(TP + FN)}
      $$

      

   2. 其中`TP` 表示真正的正类样本，`FN` 表示被错误地预测为负类的正类样本。召回率衡量了分类器对于所有真正正类样本的检测能力。


在这两个指标中，存在一个权衡关系：

- 如果我们**增加分类器的决策阈值**，即要求**更高的置信度**才将样本预测为正类，这将减少 `FP`（将负类错误地预测为正类）的数量。由于 `TP` 和 `FP` 都在分母中，减少 `FP` 将提高精确度。然而，这也可能导致 `FN`（将正类错误地预测为负类）的数量增加，从而降低召回率。

- 相反地，如果我们**降低分类器的决策阈值**，即**降低对样本预测为正类的要求**，这将减少 `FN` 的数量，提高召回率。但同时也可能增加 `FP` 的数量，降低精确度。

因此，精确度和召回率之间存在着一种权衡关系。在实际应用中，我们根据具体的需求和问题背景来选择更重视精确度还是召回率。对于一些应用，如垃圾邮件过滤，我们可能更关注精确度，因为将正常邮件错误地分类为垃圾邮件的后果较小。而在其他应用，如癌症诊断，我们可能更关注召回率，因为漏诊可能导致严重的后果。

因此，合理平衡精确度和召回率之间的权衡关系是根据具体问题的需求和情境来确定的。



### 3.3 `F1`分数

#### 3.3.1 `P-R`曲线

通过分析**精确率**和**召回率**的公式，可以发现：精确率和召回率的分子是相同的，都是`TP`，但分母是不同的，一个是`(TP+FP)`，一个是`(TP+FN)`。

如果我们把**精确率**（Precision）和**召回率**（Recall）之间的关系用图来表达，就是下面的`PR`曲线：

![img](https://pic3.zhimg.com/80/v2-2c4f47f257fb72d5e0e58789ac3c77ca_720w.webp)



#### 3.3.2 如何理解`P-R`（查准率-查全率）曲线

问题：**`P-R`曲线是根据什么变化的？为什么是这个形状的曲线？**

这要从 **排序型模型** 说起：拿逻辑回归为例，逻辑回归的输出是一个 `[0,1]` 区间的概率数字，因此，如果我们想要根据这个概率进行预测分类的话，就必须定义一个 **阈值**。

通常来说，逻辑回归的概率越大说明越接近1，也就可以预测它是正类的可能性更大。

比如，我们定义了阈值为0.5，则 $f_{\mathbf{w},b} \ge 0.5$ 时预测 $y=1$，而$f_{\mathbf{w},b} \lt 0.5$ 时预测 $y=0$；则对于阈值为0.5的情况下，我们可以得到相应的一对查准率和查全率。

但问题是：**阈值是我们随便定义的，我们并不知道这个阈值是否符合我们的要求**。

因此，**为了找到一个最合适的阈值** 满足我们的要求，我们就必须 **遍历 `[0,1]` 之间所有的阈值**，而每个阈值下都对应着一对查准率和查全率，从而就得到了上述的 `P-R`曲线。



#### 3.3.3 如何找到最好的阈值点

首先，需要说明的时我们对于这两个指标的要求：**我们希望查准率和查全率同时都非常高**。

但实际上这两个指标是一对矛盾体，无法做到同时双高。从`P-R`曲线中可以明显看到，如果其中一个非常高，则另一个肯定会非常低。

**选取合适的阈值** 要根据实际需求，比如我们想要高的查全率，那么就需要牺牲一些查准率，在保证查全率最高的情况下，查准率也不那么低。



#### 3.3.4 `F1`分数

**精确率**和**召回率**是【两难全】的关系，为了综合两者的表现，需要在两者之间找到一个平衡点，这就出现了`F1`分数：
$$
F1 = \frac{2PR}{P+R}
$$
![img](https://pic4.zhimg.com/80/v2-cdab484b4f23fa03e707d5cab00995e7_720w.webp)



### 3.4 `ROC`、`AUC`曲线

#### 3.4.1 灵敏度、特异度（真正率、假正率）

在正式介绍`ROC/AUC`之前，需要再介绍两个指标，**这两个指标的选择也正是`ROC`和`AUC`可以无视样本不平衡的原因**。这两个指标分别是：**灵敏度和（1-特异度），也叫做真正率（`TPR`）和假正率（`FPR`）**。

- $$
  灵敏度（Sensitivity） =  \frac{TP}{TP+FN}
  $$

  - 其实可以发现**灵敏度和召回率是一模一样的**，只是名字换了而已

- 
  $$
  特异度（Specificity） = \frac{TN}{FP+TN}
  $$

  - 由于我们比较关心正样本，所以需要查看有多少负样本被错误地被预测为正样本，所以使用（1-特异度），而不是特异度



- $$
  真正率（TPR）=灵敏度 = \frac{TP}{TP+FN}
  $$

- $$
  假正率（FPR） = 1-特异度= \frac{FP}{FP+TN}
  $$

![img](https://img.6aiq.com/e/38e499f5bedb42eca681a4a30c737027.jpeg)

上图是真正率和假正率的示意图，可以发现**`TPR`和`FPR`分别是基于实际表现1和0出发的，也就是h说，它们分别在实际的正样本和负样本中来观察相关概率问题**。正因为如此，所以无论样本是否平衡，都不会被影响。

拿之前的例子，在总样本中，如果90%是正样本，10%是负样本，我们知道使用**准确率**是有水分的，但是用`TPR`和`FPR`不一样：

- 这里，`TPR`只关注90%正样本中有多少是被真正覆盖的，而与那10%毫无关系
- 同理，`FPR`只关注10%负样本中有多少是被错误覆盖的，也与那90%毫无关系

即：

- **`TPR`计算的是分类器成功预测为正类的样本占所有真正正类样本的比例。它关注的是分类器再真正正类样本中的准确率，而不受负类样本的影响。**
- **`FPR`计算的是分类器错误预测为正类的负类样本占所有真正父类样本的比例。它关注的是分类器在真正负类样本中中的错误率，而不受正类样本的影响。**

因此，`TPR`和`FPR`的计算仅仅依赖于分类器的预测结果和实际标签，不考虑样本的平衡性。

所以可以看出：**如果我们从实际表现的各个结果角度出发，就可以避免样本不平衡的问题了，这也是为什么选用`TPR`和`FPR`作为`ROC/AUC`的指标的原因。**



或者我们也可以从另一个角度考虑：**条件概率**。

我们假设 $X$ 为预测值， $Y$ 为真实值，那么就可以将这些指标按条件概率表示：

- $$
  精确率 = P(Y=1 |X=1)
  $$

- $$
  召回率=灵敏度=P(X=1|Y=1)
  $$

- $$
  特异度=P(X=0|Y=0)
  $$

从上述三个公式可以看到：**如果我们先以实际结果为条件（`召回率`、`特异度`），那么就只需要考虑一种样本，而先以预测值为条件（`精确率`），那么需要同时考虑正样本和负样本，所以先以实际结果为条件的指标都不受样本不平衡的影响，相反以预测结果为条件的就会受到影响**。



#### 3.4.2 `ROC`（接受者操作特征曲线）

##### 3.4.2.1 `ROC`曲线

> `ROC`（Receiver Operating Characteristic）曲线，又称为接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力，`ROC`曲线是基于混淆矩阵得出的。

`ROC`曲线中的主要两个指标就是**真正率**和**假正率**，上面也解释了这么选择的好处所在：

- 横坐标为**假正率（`FPR`）**
- 纵坐标为**真正率（`TPR`）**
- 在`ROC`曲线上，每个点代表了分类器在特定分类阈值下的`TPR`和`FPR`

> 通过在不同的分类阈值下计算`TPR`和`FPR`，可以得到`ROC`曲线上的多个点。
>
> **注意**：
>
> 1. 在实际应用中，可以根据数据集的大小和计算资源的限制来确定分类阈值的密度。较小的分类阈值间隔可以提供更详细的`ROC`曲线，但也可能需要更多的计算时间和资源。
> 2. 绘制`ROC`曲线需要使用**独立的测试集或进行交叉验证**，以避免对模型性能的过度乐观估计（因为如果使用的不是独立测试集或交叉验证，可能会由于 “数据泄露”而使得模型学习到测试数据的某些特征从而导致模型的性能评估过度乐观）。
>
> `ROC`曲线可以提供分类器在不同分类阈值下的性能全貌，帮助我们理解分类器在`TPR`和`FPR`之间的权衡。

下面就是一个标准的`ROC`曲线图：

![img](https://img.6aiq.com/e/da2338e6537043aebce95d5e0e395164.jpeg)

##### 3.4.2.2 `ROC`曲线的阈值问题

与前面的`P-R`曲线类似，`ROC`曲线也是通过 **遍历所有阈值** 来绘制整条曲线的。如果我们不断遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在`ROC`曲线图中也会沿着曲线滑动：

![44a4458a88104df3af1f5e38d822fcd3-v2296b158ebb205a2b90d05f5d2074bbe9b.gif](https://img.6aiq.com/file/2019/2/44a4458a88104df3af1f5e38d822fcd3-v2296b158ebb205a2b90d05f5d2074bbe9b.gif)

##### 3.4.2.3 如何判断`ROC`曲线的好坏

改变阈值只是不断地改变预测的正负样本数，即`TPR`和`FPR`，但是曲线本身是不会改变的。

那么如何判断一个模型的`ROC`曲线是好的呢？

这个还是要回归到我们的目的：

- `FPR`表示模型虚报的相应程度
- `TPR`表示模型预测响应的覆盖程度

我们所希望的当然是：虚报的越少越好，覆盖的越多越好。所以总结一下就是：**`TPR`越高，同时`FPR`越低（即`ROC`曲线越陡峭），那么的模型的性能就越好**。参考如下：

![aa13d3dfebbc4ed789e58e4ad69714b9-v24ea5d54254cf5f969095d5e12703974db.gif](https://img.6aiq.com/file/2019/2/aa13d3dfebbc4ed789e58e4ad69714b9-v24ea5d54254cf5f969095d5e12703974db.gif)



> - `ROC`曲线越靠近左上角，表示分类器的性能越好。左上角的点表示高`TPR`和低`FPR`，即分类器实现了较高的灵敏度和较低的误诊率。
> - 对于随机猜测的分类器，`ROC`曲线将接近对角线（$y=x$），因为在各个分类与之下`TPR`和`FPR`的比例应该相似。



##### 3.4.2.4 `ROC`曲线无视样本不平衡

前面已经对`ROC`曲线为什么可以无视样本不平衡做了解释，下面用动态图的形式再次展示以下它是如何工作的。

我们发现：**无论红蓝色样本比例如何改变，`ROC`曲线都没有影响**。

![ef5638c5261844fcbcec9cf752f8455b-v2432e490292db97f258ecf04a7b17ef1cb.gif](https://img.6aiq.com/file/2019/2/ef5638c5261844fcbcec9cf752f8455b-v2432e490292db97f258ecf04a7b17ef1cb.gif)

`ROC`曲线在评估分类器性能时可以无视样本不平衡，这是因为`ROC`曲线的构建和计算是就**真正率(`TPR`)**和**假正率（`FPR`）**，而不依赖于样本的分布情况：

1. **`ROC`曲线的计算方式**：`ROC`曲线的构建是通过在不同的分类阈值下计算`TPR`和`FPR`这些指标的计算只取决于分类器的预测结果和实际标签，而与样本的分布无关。因此，`ROC`曲线可以在样本不平衡的情况下准确地表示分类器的性能。
2. **相对比例比绝对数量更重要**：在样本不平衡的情况下，分类器的预测结果和实际标签中可能存在较大的数量差异。然而，`ROC`曲线关注的是`TPR`和`FPR`的相对比例，而不是绝对数量。因此，即使
3. 样本中的正类样本数量远远少于负类样本数量，`ROC`曲线仍然能够准确地反映分类器的性能。
4. **分类阈值的影响**：分类器的预测结果可以通过调整分类阈值来控制。在样本不平衡的情况下，选择合适的分类阈值可以使分类器在`TPR`和`FPR`之间取得平衡。`ROC`曲线提供给了在不同阈值下分类器性能的全面试图，帮助我们理解分类器在不同权衡情况下的表现。



#### 3.4.3 `AUC`（曲线下的面积）

##### 3.4.3.1 `AUC`的定义

为了计算`ROC`曲线上的点，我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做的效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为**曲线下面积（Area Under Curve）**。

如果我们连接对角线，可以发现它的面积正好是0.5。

对角线的实际含义是：**随机判断响应与不响应，正负样本覆盖率应该都是50%，表示随机效果。**

`ROC`曲线越陡越好，所以理想值就是1，一个正方形，而最差的随机判断都有0.5，所以一般`AUC`的值是在 `[0.5,1]` 之间的。



##### 3.4.3.2 `AUC`的一般判断标准

- `[0.5, 0.7)`：效果较低，但用于预测股票已经很不错了
- `[0.7, 0.85)`：效果一般
- `[0.85, 0.95)`：效果很好
- `[0.95, 1]`：效果非常好，但一般不太可能



##### 3.4.3.3 `AUC`的物理意义

曲线下面积对所有可能的分类阈值的效果进行综合衡量。曲线下面积的一种解读方式是看作模型将某个随机正类别样本排列在某个随机负类别样本之上的概率。以下面的样本为例，逻辑回归预测从左到右以升序排列：

![img](https://img.6aiq.com/e/90b76e111ab341af885ffa2be24b9ff5.jpeg)





### 3.4.4 `ROC/AUC`的Python实现

Python中我们可以调用`sklearn`机器学习库的`metrics`进行`ROC`和`AUC`的实现，简单的代码实现部分如下：

```python
from sklearn import metrics
from sklearn.metrics import auc 
import numpy as np
y = np.array([1, 1, 2, 2])  
scores = np.array([0.1, 0.4, 0.35, 0.8])  
fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
metrics.auc(fpr, tpr) 

0.75

```



```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# 假设有分类器的预测结果（scores）和对应的真实标签（labels）
# 只需要使用独立的测试集或者交叉验证集对模型进行测试，就可以得到分类器的预测结果
scores = [0.2, 0.5, 0.6, 0.8, 0.3, 0.1]
labels = [0, 1, 1, 0, 1, 0]

# 计算TPR和FPR
fpr, tpr, thresholds = roc_curve(labels, scores)

# 计算AUC
roc_auc = auc(fpr, tpr)

# 绘制ROC曲线
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()
```

> 使用`sklearn`库绘制`ROC`曲线时，由于我们有分类器的预测结果（输出的是概率值[0,1]）和真实标签，则库中会通过遍历不同的分类阈值，将分类器的预测结果与阈值进行比较从而得到不同阈值下的分类结果然后计算对应的`TPR`和`FPR`，最后利用这一系列的值绘制`ROC`曲线。

