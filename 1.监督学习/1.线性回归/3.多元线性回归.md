# 多元线性回归



### 样本矩阵

```python
X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])
y_train = np.array([460, 232, 178])
```

如上述代码所示，**样本** 存储在 `Numpy`矩阵 `X_train`中，**矩阵的每一行都代表一个样本**。

当有 $m$ 个训练样本，并且有 $n$ 个特征（即特征维度是 $n$ 时），则存储训练样本的 $\mathbf{X}$ 是一个维度为 $(m,n)$ 的矩阵：
$$
\mathbf{X} = 
\begin{pmatrix}
x_0^{(0)} & x_1^{(0)} & \cdots & x_{n-1}^{(0)} \\
x_0^{(1)} & x_1^{(1)} & \cdots & x_{n-1}^{(1)} \\
\cdots \\
x_0^{(m-1)} & x_1^{(m-1)} & \cdots & x_{n-1}^{(m-1)}
\end{pmatrix}
$$

- $x^{(i)}$ 是第 $i$ 个样本的向量表示：$x^{(i)} = (x_0^{(0)} \space x_1^{(0)} \space \cdots x_{n-1}^{(0)})$

- $x_j^{(i)}$ 是第 $i$ 个样本的第 $j$ 个元素（特征），括号中的上标表示样本编号，而下标表示元素



### 参数向量化

- $\mathbf{w}$ 是一个含有 $n$ 个元素的向量：

  - $\mathbf{w}$ 的每一个元素都是和一个特征相关联的**参数**

  - 从概念上讲，一般将 $w$ 绘制为列向量：

  - $$
    \mathbf{w} = 
    \begin{pmatrix}
    w_0 \\
    w_1 \\
    \dots \\
    w_{n-1}
    \end{pmatrix}
    $$

- $b$ 是一个标量参数（偏置量）



## 多变量模型预测

### 模型表示 

模型对多个变量的预测由**线性模型**给出（多元线性回归）：
$$
f_{w,b}(\mathbf{x}) = w_0x_0 + w_1x_1 + ...+w_{n-1}x_{n-1} + b \tag{1}
$$
或者用 **向量化** 的表示：
$$
f_{w,b}(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x} + b \tag{2}
$$
其中 $\cdot$ 表示**向量点积**



### 模型实现

#### 1.使用循环的方式实现（逐个元素的单个预测）

- 在**一元线性回归**中，我们通过将一个特征值和一个参数相乘，并加上一个偏置参数，得到了预测，即**在一元线性回归中：$\hat{y}^{i} = wx^{i}+b$**

- 在**多元线性回归**中，可以通过扩展一元线性回归中的实现方式来实现模型表示中给出的 $(1)$式，即通过循环遍历每个元素（特征）并与其对应的参数相乘，累加，最后再加上偏置参数



```python
# 通过循环遍历每个特征，实现多元线性回归模型中对单个样本的预测
def predict_single_loop(x, w, b):
    """
    single predict using linear regression
    
    Args:
    	x (ndarray): Shape (n,) example with multiple features
    	w (ndarray): Shape (n,) model parameters
    	b (scalar) : model parameter
    	
    Returns:
    	p (scalar) : prediction
    """
    n = x.shape[0]
    p = 0
    # 遍历样本x的每一个元素（特征）
    for i in range(n):
        p_i = x[i] * w[i]	# 计算每一个特征和其参数的乘积
        p = p + p_i	# 累加
   	p = p + b	# 最后再加上偏置参数b
    return p
```

调用方式：

```python
# 从训练数据中获得单个样本
x_vec = X_train[0]
print(f'x_vec shape: {x_vec.shape}', x_vec value: {x_vec})

# 对该样本进行预测
f_wb = predict_single_loop(x_vec, w, b)
print(f"f_wb shape {f_wb.shape}, prediction: {f_wb}")
```



#### 2.使用向量点积实现（向量化）

在 [模型表示](# 模型表示) 中，我们可以看到，多元线性模型不仅可以通过累加 特征与参数的乘积 并最后加上偏置参数的方式实现（即模型表示中的公式 $(1)$ ），

还可以通过**向量化**之后的样本$\mathbf{x}$、参数$\mathbf{w}$ 之间的**点积**来实现（即模型表示中的公式$(2)$）
$$
f_{w,b}(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x} + b \tag{2}
$$
这不仅可以简化代码，同时还能利用**向量运算**来加快运算速度。



```python
# 利用向量化之后的样本、参数之间的点积来实现，即通过向量运算来实现线性回归模型
def predict_single_vector(x, w, b):
    """
    single predict using linear regression
    
    Args:
    	x (ndarray): Shape (n,) example with multiple features
    	w (ndarray): Shape (n,) model parameters
    	b (scalar) : model parameter
    	
    Returns:
    	p (scalar) : prediction
    	
    """
    p = np.dot(x, w) + b
    return p
```

> 注意：由于在使用向量点积实现线性回归模型之后，模型实现的代码简单，所以一般不需要额外实现为一个单独的函数，而是直接使用



#### 3.使用矩阵乘法实现所有样本的预测

```python
def predict_matrix(X, y, w, b):
    """
    all predict using linear regression
    Args:
    	X (ndarray (m,n))	: Data, m examples witn n features
    	w (ndarray (m,))	: model parameters
    	b (scalar)			: model parameter
    Returns:
    	predict (scalar)		: prediction
    """
    # 模型预测
    predict = X @ w + b
    
    return predict
```



## 多变量的代价函数

**具有多个变量的成本函数 $J(\mathbf{w},b)$ 的方程**为：
$$
J(\mathbf{w}, b) = \frac{1}{2m}\sum_{i=0}^{m-1}(f_{w,b}(\mathbf{x}^{(i)} - y^{(i)})) \tag{3} \\
$$
其中：
$$
f_{w,b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b \tag{4}
$$
**注意**：$\mathbf{w} 和 \mathbf{x}^{(i)}$ 都是支持多个特征的向量而不是标量



#### 1.使用`for`循环实现代价函数

```python
# 使用for循环实现代价函数，即通过遍历所有的样本，计算每个样本的预测误差并累积
def compute_cost_loop(X, y, w, b):
    """
    compute cost
    Args:
    	X (ndarray (m,n))	: Data, m examples witn n features
    	y (ndarray (m,))	: target values
    	w (ndarray (m,))	: model parameters
    	b (scalar)			: model parameter
    Returns:
    	cost (scalar)		: cost
    """
    m = X.shape[0]	# 样本数量
    cost = 0.0
    # 遍历所有样本，计算每个样本的预测的误差，并累加
    for i in range(m):
        # 注意，此处使用了向量化实现的模型（即通过向量点积的方式实现线性回归模型）
        f_wb_i = np.dot(X[i], w) + b		# scalar（向量点积的结果为一个标量）
        cost = cost + (f_wb_i - y[i])**2	# scalar
   	cost = cost / (2 * m)					# scalar
    return cost
```



#### 2.使用`向量化` 实现代价函数（向量*矩阵）

```python
def compute_cost_matrix(X, y, w, b):
    """
    Computes the gradient for linear regression
    Args:
    	X (ndarray (m,n))	: Data, m examples witn n features
    	y (ndarray (m,))	: target values
    	w (ndarray (m,))	: model parameters
    	b (scalar)			: model parameter
    Returns:
    	cost (scalar)		: cost
    """
    m, n = X.shape
    
    # 模型预测(通过矩阵乘法快速计算所有样本的预测结果)
    f_wb = X @ w + b	# f_wb 是一个m*1的向量，其中每一个元素都代表一个样本的预测结果
    
    # 计算损失
    total_cost = (1/(2*m)) * np.sum((f_wb - y)**2)
    
    return total_cost
```



## 多变量的梯度下降

**多个变量的梯度下降**：
$$
\text{repeat until convergence: \{}
\\
w_j = w_j - \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \space \space \space \space  \text{for j = 0..n-1} \tag{5} \\
b = b - \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\
\text{\}}
$$
公式中：

-  $n$ 是特征的数量，$m$ 是训练集中的样本数量；

- $$
  \frac{\partial J(\mathbf{w},b)}{\partial w_j} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  \tag{6} \\
  $$

- $$
  \frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})  \tag{7} \\
  $$

  

- $f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}$ 是模型的预测值，而$y^{(i)}$ 是目标值（标签）

**注意**：所有的参数 $w_j$ 和 $b$ 都是同时更新的。



#### 使用多个变量计算梯度

##### 1.使用`for`循环实现：

实现计算公式$(6)$和$(7)$有多种方式，以下是其中一种实现方式：

- **外层循环遍历所有样本**：
  - 首先计算该样本的预测误差 `err`
  - $\frac{\partial J(\mathbf{w},b)}{\partial b}$ 是对每一个样本的预测误差的累加，所以可以直接在外层循环里进行累加运算即可
  - **内层循环遍历所有特征**：
    - 对每一个特征相对应的参数 $w_j$ 来说，$\frac{\partial J(\mathbf{w},b)}{\partial w_j}$ 是 $(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}$ 的累加，所以在内层循环中，对每一个$w_j$的梯度，都应该加上对应的值



> 有另一种实现方式：
>
> - 外层循环遍历所有参数
>   - 内层循环遍历所有样本
>     - 内层循环里首先计算出该样本的预测误差 `err`
>     - 然后是对外层循环的每一个参数的梯度 $w_j$ 加上 $(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}$
>
> 但是这种实现方式，需要计算 m*n 次预测误差 `err`，而第一种实现方式中只需要计算 `m`次预测误差 `err`，所以效率更高



```python
def compute_gradient_loop(X, y, w, b):
    """
    compute the gradient for linear regression
    Args:
    	X (ndarray (m,n))	: Data, m examples with n features
    	y (ndarray (m,))	: target values
    	w (ndarray (n,))	: model parameters
    	b (scalar)			: model parameter
    Returns:
    	dj_dw (ndarray (n,)) :The gradient of the cost w.r.t the parameters w.
    	dj_db (scalar)		 :The gradient of the cost w.r.t the parameter b.
    """
    m, n = X.shape # (number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.
    for i in range(m):
        err = (np.dot(X[i], w) + b) - y[i]	# 计算第i个样本的预测误差
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err * X[i,j]
        dj_db = dj_db + err
   	
    dj_dw = dj_dw / m
    dj_db = dj_db / m
    
    return dj_db, dj_dw
```

调用方式：

```python
#Compute and display gradient 
tmp_dj_db, tmp_dj_dw = compute_gradient_loop(X_train, y_train, w_init, b_init)
print(f'dj_db at initial w,b: {tmp_dj_db}')
print(f'dj_dw at initial w,b: \n {tmp_dj_dw}')
```



##### 2.使用向量化的方式实现：

```python
def compute_gradient_matrix(X, y, w, b):
    """
    Computes the gradient for linear regression
    Args:
    	X (ndarray (m,n))	: Data, m examples with n features
    	y (ndarray (m,))	: target values
    	w (ndarray (n,))	: model parameters
    	b (scalar)			: model parameter
    Returns:
    	dj_dw (ndarray (n,)) :The gradient of the cost w.r.t the parameters w.
    	dj_db (scalar)		 :The gradient of the cost w.r.t the parameter b.
    """
    m, n = X.shape		#(number of examples, number of features)
    # 模型预测
    f_wb = X @ w + b	#利用矩阵*向量，快速计算所有样本预测结果（结果为一个 m*1的向量
    err = f_wb - y		#计算所有预测结果的误差（使用了向量减法 或称为矩阵减法
    
    # dj_dw即参数w的梯度（注意w是一个向量）， X.T @ err 得到的结果为 (n*1)的向量
    # X.T @ err 实现了公式(6)中的累加部分的工作（参考矩阵乘法）
    dj_dw = (1/m) * (X.T @ err)
    
    # dj_db即参数b的梯度，通过累加所有预测结果的误差然后除以m可计算出来
    dj_db = (1/m) * np.sum(err)
    
    return dj_db, dj_dw
```



#### 具有多个变量的梯度下降

在实现了计算梯度的函数之后，便可以实现梯度下降的函数了，即在实现了公式 $(6)$ 和 $(7)$之后，便可以代入到公式 $(5)$ 中计算：



```python
def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):
    """
    Performs batch gradient descent to learn theta. Updates theat by taking num_iters gradient steps with leaning rate alpha.
    
    Args:
    	X (ndarray (m,n))	: Data, m examples with n features
    	y (ndarray (n,))	: target values
    	w_in (ndarray (n,))	: initial model parameters
    	b_in (scalar)		: initial model parameter
        cost_function		: function to compute cost
        gradient_function`	: function to compute gradient
        alpha (float)		: Learning rate
        num_iters (int)		: number of iterations to run gradient descent
        
   	Returns:
   		w (ndarray (n,))	: Updated values of model parameters
   		b (scalar)			: Updated value of model parameter
    """
    
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w = copy.deepcopy(w_in)		# avoid modifying global w within function
    b = b_in
    
    for i in range(num_iters):
        
        # calculate the gradient and update the parameters
        dj_db,dj_dw = gradient_function(X, y, w, b)
        
        # update parameter using w, b, alpha and gradient
        w = w - alpha * dj_dw
        b = b - alpha * dj_db
        
        # save cost J at each iteration
        if i < 100000:		# prevent resource exhausition
            J_history.append(cost_function(X, y, w, b))
        
        # print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}		")
 	
    return w, b, J_history	# return final w, b and J history for graphing
```



调用方式：

```python
# initialize parameters
initial_w = np.zeros_like(w_init)
initial_b = 0.
# some gradient descent settings
iterations = 1000
alpha = 5.0e-7
# run gradient descent 
w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,compute_cost, compute_gradient,alpha, iterations)
print(f"b,w found by gradient descent: {b_final:0.2f},{w_final} ")
m,_ = X_train.shape
for i in range(m):
    print(f"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}")
```

