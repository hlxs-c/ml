# 梯度下降

1.**梯度下降是一个用来求函数最小值的算法**，我们使用**梯度下降算法**来求解使**代价函数$J(w,b)$ **达到最小值的参数 $w,b$。

2.梯度下降背后的思想是：首先，随机选择一个参数的组合 $(w_0,b_0)$，计算代价函数，然后寻找下一个能让代价函数值下降最多的参数组合，持续这么做直到找到一个局部最小值（**local minimum**）。

> 因为并没有尝试所有的参数组合，所以不能确定得到的局部最小值是否是全局最小值（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。

![image-20231120093106486](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120093106486.png)

想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。



2.批量梯度下降（**batch gradient descent**）算法的公式为：

repeat until convergence {

​	$ w_j = w_j - \alpha \frac{\delta}{\delta w_j}J(w,b)$

​	$b = b - \alpha \frac{\delta}{\delta b}J(w,b)$

}

其中 $\alpha$ 是学习率（**learning rate**），它决定了我们能沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降算法中，我们每一次都**同时**让所有的参数减去学习率乘以代价函数的导数值，即 **同时更新参数**：

![image-20231120093956876](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120093956876.png)



3.对学习率的选择：

- 如果$\alpha$太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果$\alpha$太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。

- 如果$\alpha$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$\alpha$太大，它会导致无法收敛，甚至发散。

![image-20231120094808270](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120094808270.png)

实例（当学习率$\alpha$ 太大时，梯度下降算法越过最低点，最后甚至无法收敛）：

![image-20231120101144394](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120101144394.png)





4.如果初始参数就在局部最小值，则梯度下降算法的工作：

- 如果初始参数在局部最低点，则因为在局部最低点处的导数为0（导数是该点的斜率，局部最低点的切线为一条水平线），这意味着在局部最低点处，梯度下降算法不会改变参数（减去一个0不会改变参数的值。
- 因此，当参数已经处于局部最优点，则梯度下降算法更新其实什么都没做，他不会改变参数的值。

![image-20231120094825747](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120094825747.png)



5.即使学习速率$\alpha$ 保持不变，梯度下降算法也可以收敛到局部最低点：

- 因为在梯度下降算法中，当我们越来越接近局部最低点时，导数值的绝对值会越来越小（越来越接近于0，因为局部最低点的导数值为0），所以在接近局部最低点时，导数值会自动变得越来越小，则梯度下降将自动采取较小的幅度（即更新参数的幅度），所以即使学习率 $\alpha$ 是固定的，梯度下降算法最终也可以收敛到局部最低点。

![image-20231120094832285](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120094832285.png)

实例：

![image-20231120100811297](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120100811297.png)

在上图中，环表示代价函数的等值线，红色箭头表示的是梯度下降的路径，可以看到，梯度下降逐步向全局最低点（即代价函数值最小的点）靠近，且初始步长比目标附件的步长大很多，放大看如下图所示：

![image-20231120101034815](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120101034815.png)





6.线性回归中的梯度下降：

![image-20231120095318888](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120095318888.png)



梯度推导：

![image-20231120095339489](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120095339489.png)



线性回归中的梯度下降：

![image-20231120095350172](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120095350172.png)

注意：要**同时更新**参数



![image-20231120095428931](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231120095428931.png)

