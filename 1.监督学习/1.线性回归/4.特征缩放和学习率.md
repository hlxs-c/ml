# 特征缩放和学习率



## 1.查看数据集及其特征

在有多个特征的数据集中，可能并不是所有的特征都是有效的，或者说有些特征对目标值的影响很小，我们可以**通过绘制每个特征与目标值的图** 来查看数据集及其特征：

例如，在一个用来训练预测房价的模型的数据集中，其每个样本都有4个特征（大小、卧室数量、楼层和年龄），如下表所示：

![image-20231122164650016](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122164650016.png)

如果想要知道**哪一个特征对目标值的影响较大**，则可以**通过沪指每个特征与目标值的图** 来查看数据集及其特征，在本例中，目标值为房价：

![image-20231122164803696](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122164803696.png)

如上图所示，通过将每个特征与目标价格进行对比，可以指示出哪些特征对价格的影响最大，从上图中可以看出，增加尺寸会增加价格，而卧室数量和楼层则似乎对价格没有太大的影响，新房子的价格比旧房子高。



## 2.确保梯度下降算法工作正常的方法

**调试**：确保 梯度下降算法 是正常工作的

一种常用的方法是：**在梯度下降算法运行时，绘制出 代价函数 $J(\mathbf{w}, b)$ 的值随梯度下降算法迭代次数的变化曲线**

![image-20231122170249444](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122170249444.png)



通过绘制该图，图中的曲线可以显示 **梯度下降的每次迭代之后，代价函数 $J(\mathbf{w},b)$ 的值**

如果梯度下降算法正常工作的化，那么在每一次迭代之后，$J(\mathbf{w},b)$ 的值都应该在减小（曲线呈现下降趋势）；

同时，由于梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，是无法提前预知的，所以也可以通过绘制该图，利用 **代价函数随梯度下降算法的迭代次数变化的曲线** 帮助判断梯度下降算法是否在逐步收敛；



除了上述通过 **代价函数随梯度下降算法的迭代次数变化的曲线** 来判断梯度下降算法是否在收敛，还可以通过 **自动收敛测试** 的方法来判断：

**自动收敛测试** 的一个非常典型的例子：如果代价函数 $J(\mathbf{w},b)$ 在每一次迭代之后的下降小于一个很小的值 $\epsilon$ ，则这个测试就判断梯度下降算法已经收敛，这个阈值 $\epsilon$ 在某个问题中可以是 $0.0001$，但通常要选择一个合适的阈值 $\epsilon$ 是非常困难的，因此，为了检查梯度下降算法是否收敛，一般来说更倾向于查看 **代价函数随梯度下降算法的迭代次数变化的曲线**， 而不是依靠自动收敛测试。



## 3.学习率的选择

通过查看 **代价函数随梯度下降算法的迭代次数变化的曲线**， 可以判断梯度下降算法是否在正常工作：

- 如果曲线呈现一个上升趋势（即随着迭代次数增加代价函数的值越来越大），如下图所示：

  ![image-20231122171914924](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122171914924.png)

  则说明梯度下降算法没有正常工作

  1. 遇到这种情况，通常意味着需要使用更小的**学习率$\alpha$** （因为当学习率太大时，而初始参数临近最优参数值时，学习率太大可能会导致梯度下降太大从而使得代价函数的值变大），如下图所示：

     ![image-20231122171433153](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122171433153.png)

     在这种情况下，解决方法是 **使用更小的学习率$\alpha$**。

  2. 遇到这种情况还有另一种可能性：代码中有错误，例如在使用梯度下降算法更新参数时的符号使用错误：

     - 正确的使用：$w_j = w_j - \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}$
     - 错误的使用：$w_j = w_j + \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}$，这种错误的使用会导致随着梯度下降算法的迭代，参数$w_j$ 逐渐原理最优参数值，从而使得随着迭代次数的增加代价函数的值也越来越大。

- 如果曲线呈现波动趋势，如下图所示：

  ![image-20231122172001628](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122172001628.png)

  也说明梯度下降算法没有正常工作，可能的原因也是两种：

  1. 学习率太大
  2. 代码中存在错误



**注意**：

- 只要**学习率$\alpha$ **足够小，则每次迭代之后代价函数的值都应该会减小
- 但是并不是学习率$\alpha$ 越小越好，学习率太小会导致梯度下降算法收敛的次数很大则需要花费较长的时间进行迭代

![image-20231122172223869](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122172223869.png)



**总结**：

1.当学习率太小时，梯度下降算法收敛非常缓慢。

2.当学习率太大时，代价函数可能会在每次迭代之后增大，或者代价函数的值来回振荡，甚至是不收敛（发散）。

3.代价函数的值随着梯度下降算法迭代次数的变化曲线是一个较好的工具。

4.学习率的尝试（每次增加3倍）：

![image-20231122172520391](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122172520391.png)



**实例说明：**

```python
#set alpha to 9.9e-7
_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)
```

在多元线性回归中，使用学习率 $\alpha=9.9e=7$ 进行训练，在训练过程中打印相关的值：

![image-20231122172914788](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122172914788.png)

从上图中可以看到，梯度下降算法没有收敛，代价函数的值不但没有减少，反而增加，这说明学习率太高了；

绘制代价函数的值随着梯度下降算法迭代次数的变化曲线：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122173044521.png" alt="image-20231122173044521" style="zoom:50%;" />

再绘制出代价函数在其他参数固定，而参数$w_0$ 不固定的曲线，并在其上绘制出在迭代过程中随参数 $w_0$ 的变化时代价函数的值的变化：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122173329638.png" alt="image-20231122173329638" style="zoom:50%;" />

上图中显示了在每次迭代中，参数$w_0$ 的值都会越过最佳值，并逐渐远离最佳值，从而代价函数的值也越来越大。



减小学习率为：$\alpha=9e-7$，再次训练并绘制相关图形，结果如下，可以看到代价函数的值随迭代次数增加逐渐减小并收敛，而参数 $w_0$也逐渐逼近最佳值。

![image-20231122173530204](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122173530204.png)



## 4.特征缩放

[特征缩放](C:\Data\Markdown\MachineLearning\other\特征缩放.md)

**特征缩放的重要性：使所有特征具有相似的取值范围。**



#### 1.特征的不同取值范围导致的参数最优化速度的不同

以房价预测为例，训练集中每个样本的特征维度为4，分别为（大小、卧室数量、楼层、年龄），数据集如下图所示：

![image-20231122192845976](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122192845976.png)

可以看到，这4个特征的取值范围是不一样的，且 `大小` 这个特征的取值范围和其他特征的取值范围差别很大。

接下来我们使用这个数据集进行 `多元线性回归`模型的训练，以下展示了一个简短的运行（迭代次数较少），显示了前几次迭代：

![image-20231122193132590](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122193132590.png)

如上图所示，可以看到**代价函数的值在初始降低（首次降低）之后开始缓慢下降**。

并且注意到 $w_0$ 和 $w_1,w_2,w_3$ 之间 以及 $djdw_0$和 $djdw_1,djdw_2,djdw_3$ 之间的区别：

- $w_0$ 很快就接近了最终值（如上图所示在第3次迭代之后 $w_0$ 的值就没有再变化了，也就是说 $w_0$ 仅用了3次迭代就达到了最终值——也即在该模型以及**训练超参数**下的最优值），且 $djdw_0$ 迅速下降到一个很小的值，表明 $w_0$ 接近最终值。
- 其他参数的减少速度要慢很多。



#### 2.特征的不同取值范围导致参数最优化速度不同的原因

![image-20231122194808539](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231122194808539.png)

如上图所示：在梯度下降算法中，
$$
\text{repeat until convergence: \{}
\\
w_j = w_j - \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \space \space \space \space  \text{for j = 0..n-1} \tag{5} \\
b = b - \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\
\text{\}}
$$

$$
\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  \tag{6} \\
$$

$$
\frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})  \tag{7} \\
$$



在公式中可以看到，在使用梯度下降算法更新参数时，**学习率$\alpha$** 对所有的参数$w_j$来说都是相同的，且在 $\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \frac{1}{m}\sum_{i=0}^{m-1}(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  \tag{6} \\$ 中，$(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})$ 项对于所有的参数$w_j$来说也是相同的（预测误差），而有所不同的是其中的 **项$x_j^{(i)}$**，在更新不同的参数$w_j$ 时，**项 $x_j^{(i)}$** 各不相同，所以当不同特征的取值范围不同且相差很大时，在更新不同的参数 $w_j$时，**项$x_j^{(i)}$** 就会相差很大，从而导致不同特征的**更新幅度** 也会相差很大，也就会造成 **使用梯度下降算法更新参数时更新不均匀的现象**。

> 总结：当特征的取值范围的量级差异很大时，就会使得特征对应的参数的更新速度不同：
>
> - 特征的值越大，更新速度越快，这是因为当特征的值越大，则更新参数时的**项 $x_j^{(i)}$**就会越大，从而更新幅度就会越大，更新速度就会越快，就会越快接近最终值；
> - 特征的值越小，更新速度越慢；

例如在上述示例中，由于 `大小` 这个特征的取值范围远远大于其他参数的取值范围，所以在使用梯度下降算法更新参数时，更新 `大小`这个特征对应的参数$w_0$ 的 **更新幅度** 也就会远远大于其他参数。





解决 **更新速度不均匀**的方法是——**特征缩放**。



#### 3.特征缩放 解决更新速度不均匀的问题

**特征缩放，实质上是将每个特征除以用户选择的值，得出介于 [-1, 1] 之间的取值范围。**

> Feature scaling, essentially dividing each feature by a user selected value to result in a range between -1 and 1.

**特征缩放的思想：保证所有的特征的取值范围在一个相似的范围**。



在课程中提到三种不同的**特征缩放技术**：

1. 除以最大值

2. **均值归一化**（`Mean normalization`）：$x_i = \frac{x_i - \mu_i}{max_i-min_i}$

   - 其中 $x_i$ 表示第$i$个特征，$\mu_i$ 表示数据集中第$i$ 个特征的取值的平均值，而$max_i$ 和$min_i$ 则分别是数据集中第$i$ 个特征的最大值和最小值

3. **`z-score` 归一化/标准化**（`Z-score normailzation`）：
   $$
   x_i = \frac{x_i-\mu_i}{\sigma_i}
   $$

   - 其中 $x_i$ 表示第$i$个特征，$\mu_i$ 表示数据集中第$i$ 个特征的取值的平均值，$\sigma_i$ 是数据集中第$i$个特征的所有取值的标准差：

     - $$
       \mu_i = \frac{1}{m}\sum_{j=0}^{m-1}x_i^{(j)} \\
       即将所有样本中第i个特征的值相加，最后除以样本数量\\
       \sigma_{i}^2 = \frac{1}{m}\sum_{j=0}^{m-1}(x_{i}^{(j)}-\mu_i)^2 \\
       即将每个样本中第i个特征的取值与其均值的差的平方相加，最后除以样本数量
       $$

       

   - 在经过`z-score`归一化后，特征的**均值为0，标准差为1**

   - **注意**：在实现**`z-score`归一化** 时，必须存储用于归一化的值，即存储用于归一化计算时的 **平均值$\mu_i$** 和 **标准差$\sigma_i$**。因为在训练模型之后（得到较优的参数），需要使用模型进行预测。在给定一个新的样本$x$输入时，由于训练模型时对样本输入进行了特征缩放（归一化），则在进行预测时，也必须通过从训练集中计算得到的**平均值**和**标准差** 来对新的样本$x$ 输入进行归一化，然后再输入到模型中进行预测，才能保证输入满足模型的要求。

   - 实现：

     - ```python
       def zscore_normalize_features(X):
           """
           computex X, zscore normailized by column
           Args:
           	X (ndarray (m,n)): input data, m examples, n features
           Returns:
           	X_norm (ndarray (m,n)): normailzed X by column
           	mu (ndarray (n,)): mean of each feature
           	sigma (ndarray (n,)): standard deviation of each feature
           """
           # find the mean of each column/feature
           mu = np.mean(X, axis = 0)	# mu will have shape (n,)
           # find the standard deviation of each feature
           sigma = np.std(X, axis = 0)	# sigma will have shape (n,)
           # element-wise, subtract mu for that column from each example, divied by std for that column，利用了广播机制
           X_norm = (X - mu) / sigma
           
           return (X_norm, mu, sigma)
       ```

       



让我们来看一下 `Z-score normalizaition` 中涉及到的步骤，下图显示了逐步转换：

![ ](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123101800943.png)

上图显示了训练集中`size`和`age` 两个特征之间的关系（以相等的比例进行绘制）：

- 左图：未归一化，`size`（平方英尺）特征的取值范围或方差远远大于年龄
- 中间：第一步查找将每个特征减去其对应的平均值，这样一来，特征的取值就以0为中心
  - 其中可以很明显地看到在经过 `z-score normalization` 的第一步（减去平均值）之后，`size` 这个特征的取值显然分布在0的附近，即以0为中心
- 右图：第二部除以标准差（归一化之后），可以看到，此时`size` 和 `age` 两个特征都以0为中心，且具有相似的比例



**将归一化后的数据与原始数据进行比较**：

```python
# normalize the original features
X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)
print(f"X_mu = {X_mu}, \nX_sigma = {X_sigma}")
print(f"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}")   
print(f"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}")
```

![image-20231123102408038](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123102408038.png)

通过归一化，每根色谱柱的峰间范围从数千倍减小到2-3倍。



<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123102536983.png" alt="image-20231123102536983" style="zoom:80%;" />

如上图显示，归一化后数据的范围以0为中心，大致为 [-1,1]，最重要的是，每个特征的取值范围都是相似的。



#### 4.特征缩放加快梯度下降算法的收敛

1.另一种查看特征缩放的方法是根据成本等值线，当特征取值范围比例不匹配时，等值线图中的成本与参数的关系图是不对称的。

例如，在房价预测中，`size`特征的取值范围远远大于 `bedrooms`特征的取值范围，根据上文[更新不均匀](# 2.特征的不同取值范围导致参数最优化速度不同的原因 ) 的叙述，则会导致 `size` 对应的参数 $w_1$ 和 `bedrooms` 对应的参数 $w_2$ 的更新速度不一致，则成本等值线关于这两个特征则是一个 **椭圆**，在使用梯度下降时不一定是朝着最优点（圆心）前进的（**可能会发生振荡**），则收敛速度就慢；

而在使用特征缩放之后，特征的取值范围相近，则成本等值线关于这两个特征则会尽可能地趋近于元，因此梯度下降时会一直朝着最优点（圆心）前进，则收敛速度就快；

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123103546570.png" alt="image-20231123103546570" style="zoom:80%;" />



实例：

![image-20231123104621512](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123104621512.png)

在没有进行特征缩放时，由于 `size`的取值范围远远大于 `bedrooms`的取值范围，则成本等值线关于参数 $w_0$ 和 $w_1$ 的图形就是一个椭圆（如上图中右图所示，椭圆太长，只展示了其中一部分），而在经过特征缩放之后（使用了`z-score normalization`），成本等值线关于这两个参数的图虽然依旧为椭圆形，但已经比左图远远接近于一个圆形。

 

在经过特征缩放之后，训练的模型的预测结果（较为良好）：

![image-20231123105029470](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123105029470.png)

**注意**：**因为有多个特征，所以需要显示 `预测结果` 与 不同`特征` 的图**



#### 5.有需要的特征缩放

1**特征缩放** 的目的是使所有特征的取值范围在一个相似的范围之内，而不是要求完全相同，所以并不需要严格限制特征的取值范围为 $[-1,1]$。

2.在执行特征缩放时，通常是将所有特征的取值约束到 [-1,1] 范围内。

但并不需要严格地限制在 [-1,1] 范围内，在 [-1,1] 相近的范围都是可以的，因为最主要的目的是为了使所有的特征都处在相近的取值范围。 例如：

- 有特征 $x1∈[0,3]$，特征 $x2∈[-2,0.5]$，这些都是可以的（即可以不进行特征缩放），因为都与 [-1,1]这个范围相近。

- 但是如果有特征 $x3∈[-100,100]$，这个范围跟[-1,1]就相差甚远了（大很多），有很大的不同，所以就需要进行特征缩放；

- 或者如果有特征 $x4∈[-0.0001,0.0001]$，同样这个范围和[-1,1]相差也比较大（小很多），所以也被认为需要进行特征缩放。

总的来说，就是一个特征的取值范围可以不严格在[-1,1]之间，但不能太大，也不能太小。

不同的人有不同的经验和选择，但**一般可以这样考虑**：如果一个特征是在 $[-3,3]$的范围内，则这个范围是可以接收的，如果取值范围比这个大，则需要考虑进行“特征缩放”；同样的，如果一个特征是在$[-\frac{1}{3},\frac{1}{3}]$的范围内，同样也是可以接收的，但是如果取值范围比这个还要更小，则也需要考虑进行“特征缩放”。

**不需要严格地确保所有特征的取值范围都是相同的**，但是要确保所有特征的取值范围相近，则可以使得梯度下降算法正常进行。