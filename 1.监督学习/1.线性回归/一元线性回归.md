# 一元线性回归

> 单变量线性回归（Linear Regression with One Variable）



## 1.模型表示

1.一些符号概念：

- $m$ 代表须训练集中样本的数量
- $x$ 代表特征/输入变量
- $y$ 代表目标变量/输出变量（标签）
- $(x,y)$ 代表训练集中的实例（样本）
- $(x^{(i)},y^{(i)})$ 代表第 $i$ 个观测实例（样本）
- $f_{(w,b)}(x)$ 或 $f(x)$ 代表学习算法的解决方案或函数或模型
  - 在 **单变量线性回归模型** 中，$f_{(w,b)} = wx^{(i)+b}$，这是一个从 $x$ 映射到 $y$ 的函数

![image-20231119190450276](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231119190450276.png)

![image-20231119190513827](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231119190513827.png)





## 2.代价函数

1.模型所预测的值于训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）

![img](http://www.ai-start.com/ml2014/images/6168b654649a0537c67df6f2454dc9ba.png)

目标就是要选择出可以使得 **误差** 最小的模型参数，既然要使 **误差** 最小化，则就需要使用某个函数来代表（度量）**误差**，从而引出 **代价函数**。



**代价函数用于衡量预测值与训练数据的匹配程度**。



2.在 **单变量线性回归** 中，使用的 **代价函数** 是 **平方误差函数**，之所以要求出误差的平方和，是因为**误差平方函数** 对于大多数问题，特别是 **回归问题**， 都是一个合理的选择。

> 还有其他的代价函数也可以很好地发挥作用，但是平方误差函数可能是解决回归问题最常用的手段。

![image-20231119191802230](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231119191802230.png)

如上图所示，在 **单变量线性回归**中，使用的**代价函数** 是 **平方误差函数(`Squared error cost function`)**——
$$
J(w,b) = \frac{1}{2m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})^2 \\ 
=\frac{1}{2m}\sum_{i=1}^m(f_{(w,b)}(x^{(i)}) - y^{(i)})^2 \\
$$

- $f_{(w,b)}(x^{(i)})$ 是我们在参数为 $w,b$ 时的对第$i$ 个样本输入的 预测值
- $(f_{(w,b)}(x^{(i)}) - y^{(i)})^2$  是目标值与预测值之间的误差的平方
- 在**平方误差函数**中，我们首先会累计所有样本的误差的平方，然后除以 $2m$ 得到最终的误差，除以 $m$ 是因为要求平均值，除以 $2$ 是为了之后的求导计算可以简化（因为代价函数求导刚好可以消去$2$）

目标就是要找到参数 **$w,b$** ，使得对于每一个样本$(x^{(i),y^{(i)}})$来说，预测值 $\hat{y}^{(i)}$ 都能最接近 实际值$y^{(i)}$



3.**平方误差函数** 保证了总是有一个**全局最小值**。