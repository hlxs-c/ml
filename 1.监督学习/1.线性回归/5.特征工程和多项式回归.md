# 特征工程和多项式回归

使用**特征工程** 和 **多项式回归**，**将允许我们使用线性回归机制来拟合非常复杂甚至非线性的函数**。



线性回归提供了一种构建以下模型的方法：
$$
f_{w,b} = w_0x_0 + w_1x_1 + ...+w_{n-1}x_{n-1} + b \tag{1}
$$
但是当`特征/数据`是**非线性的**或者 **特征之间有相关的联系**（例如，某个特征是其他特征的组合），则可能会导致目标值与特征并不是线性关系，而是曲线，此时该如何使用线性回归来拟合曲线来搭建模型？

- 注意，在线性回归模型的训练中，所作的工作是修改/更新参数 $\mathbf{w}, b$ 以使根据方程$(1)$建立的模型更好的拟合数据。
- 但是，无论如何修改/更新参数 $\mathbf{w},b$，我们都无法实现对非线性曲线的拟合。



## 多项式特征（实例说明）

1.构建一个非线性的场景，用线性回归模型拟合一个二次函数：
$$
y = 1+x^2
$$
创建**训练数据集**：

```python
# create target data
x = np.arange(0, 20, 1)		
y = 1 + x**2				# 目标值，标签
X = x.reshape(-1, 1)		# 训练样本
```



2.直接使用线性回归模型进行训练：

```python
model_w,model_b = run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-2)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("no feature engineering")
plt.plot(x,X@model_w + model_b, label="Predicted Value");  plt.xlabel("X"); plt.ylabel("y"); plt.legend();
```

训练结果：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123111609894.png" alt="image-20231123111609894" style="zoom:67%;" />

如上图所示，训练得到的结果并不是很合适。



从我们的假设场景（拟合二次函数）来看，我们需要的使类似 $y=w_0x_0^2+b$ 或 **多项式特征**的东西，为了实现这个，我们可以 **修改输入数据** 来 **设计新的特征**。



3.**修改输入数据** 以 **设计新的特征**：将原始数据平方，得到原始特征的平方，即使用 $x^2$ 来代替 $x$ 进行训练：

```python
# Engineer features 
X = x**2      #<-- added engineered feature

X = X.reshape(-1, 1)  #X should be a 2-D Matrix
```



重新训练：

```python
model_w,model_b = run_gradient_descent_feng(X, y, iterations=100000, alpha = 1e-5)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("Added x**2 feature")
plt.plot(x, np.dot(X,model_w) + model_b, label="Predicted Value"); plt.xlabel("x"); plt.ylabel("y"); plt.legend(); plt.show()
```

训练结果：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123112104405.png" alt="image-20231123112104405" style="zoom:67%;" />

如上图所示，训练得到的结果近乎完美拟合训练数据，也即近乎完美拟合二次函数 $y=1+x^2$。

从训练结果可知，经过训练，梯度下降算法将参数 $\mathbf{w},b$ 更新为 $(1.0, 0.049)$，或者说调整模型为 $y=1 * x^2 + 0.049$ ，非常接近我们的目标 $y=1*x^2+1$。



## 选择`特征`

在上述的实例场景中，由于我们事先知道需要一个 $x^2$ 的多项式特征，但是通常我们并不知道需要哪些特征。

一个通常的方法是：**添加各种潜在需要的 `特征`**，以尝试找到最有用的特征。

例如，在上述实例中，我们在初始建立模型时可以尝试加入 $x^2, x^3$ 的特征，即搭建模型：
$$
y = w_0x_0 + w_1x_1^2 + w_2x_2^3 + b
$$
为了建立上述模型，则需要修改原始数据以设计新的特征，根据上述模型，我们需要设计 $x^2,x^3$ 这两个新的特征，并将 $x,x^2,x^3$ 这三个特征同时作为输入来训练模型：

```python
# engineer features .
X = np.c_[x, x**2, x**3]   #<-- added engineered feature
# 上述特征工程实现了设计新的特征x^2和x^3，并将 x,x^2,x^3 三个特征同时作为样本的特征输入
```

![image-20231123113439663](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123113439663.png)

如上图所示部分样本输入，在经过**特征工程**之后，每个样本有3个特征输入，分别为 $x,x^2,x^3$ 。



重新训练：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123113621830.png" alt="image-20231123113621830" style="zoom:67%;" />

在经过训练之后，得到的参数值为 $\mathbf{w}=[0.08 0.54 0.03], b=0.0106$，这意味着拟合/训练之后的模型为：
$$
y=0.08x+0.54x^2+0.03x^3+0.0106
$$
梯度下降通过增加$w_1$的权重占比来强调 $x^2$ 时最适合数据的特征。

**梯度下降通过强调其相关参数为我们选择 “正确” 的特征**

- 权重值越小意味着特征的重要性/正确性越低，在极端情况下，当权重变为0或非常接近0时，相关特征在将模型拟合到数据时就没有什么作用

> 如上所述，拟合之后，与 特征$x^2$ 相关联的权重远远大于特征 $x$或$x^3$ 的权重，因为它在拟合数据时最有用



## 另一种选择特征的思考方式

在上面的 [选择特征](# 选择`特征` ) 中，我们是通过在初始时 **添加各种潜在的特征**，然后通过训练之后，**根据它们与目标数据的匹配程度** 来选择**多项式特征**的。

另一种思考方式是：**一旦我们创建了新的特征，我们仍然在使用线性回归，鉴于此，最佳的特征将是相对于目标的线性特征**。

例如，在使用线性回归模型拟合二次函数 $y=x^2+1$ 时，我们通过修改原始数据设计了 $x^2,x^3$ 两个新特征，但是在 $x, x^2, x^3$ 这三个特征内，我们不知道哪一个特征才是在拟合数据中发挥最大的作用，则此时，我们可以通过**绘制 `目标值` 与 每个 `特征` 的关系图**，因为我们在设计了新的特征之后，使用的依旧是**线性回归模型**，所以**最佳的特征就是相对目标值的线性特征**，也即**关系图为线性**的特征：

```python
# create target data
x = np.arange(0, 20, 1)
y = x**2

# engineer features .
X = np.c_[x, x**2, x**3]   #<-- added engineered feature
X_features = ['x','x^2','x^3']

fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X[:,i],y)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("y")
plt.show()
```

![image-20231123131446196](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123131446196.png)

从上图可以清除地看到特征$x^2$ 映射到目标值的图形是线性的。所以线性回归模型使用该特征就可以较好地构建模型。



## 特征缩放

在 [特征缩放和学习率](C:\Data\Markdown\MachineLearning\1.监督学习\1.线性回归\4.特征缩放和学习率.md) 中所述，如果数据集的特征的取值范围有明显不同的尺度，则应该使用**特征缩放** 来加速梯度下降。

而在使用 **多项式回归**时，因为我们通过修改原始数据设计了新的特征，例如设计了新的特征 $x^2,x^3$，那么此时**特征缩放** 就变得非常重要了，因为在经过平方、立方之后，特征的取值范围也会随之变化，为了在使用梯度下降时，特征的取值范围为一个可比较的值范围（例如`[-1,1]`），避免不容易收敛或不收敛的问题，所以需要进行特征缩放。



```python
x = np.arange(0,20,1)
y = x**2

X = np.c_[x, x**2, x**3]
X = zscore_normalize_features(X) 

model_w, model_b = run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("Normalized x x**2, x**3 feature")
plt.plot(x,X@model_w + model_b, label="Predicted Value"); plt.xlabel("x"); plt.ylabel("y"); plt.legend(); plt.show()
```

训练结果：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123141307120.png" alt="image-20231123141307120" style="zoom:67%;" />

可以看到：

- **特征缩放可以更快地收敛**

- 在最终得到的参数 $\mathbf{w}, b$ 中，$x^2$ 这个特征对应的参数 $w_1$ 占比最大，而$x,x^3$ 对应的参数（权重）接近0，则说明梯度下降几乎消除了 $x,x^3$ 项。



## 拟合复杂函数

通过**特征工程**，甚至可以对相当复杂的函数进行建模：

```python
x = np.arange(0,20,1)
y = np.cos(x/2)

X = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]
X = zscore_normalize_features(X) 

model_w,model_b = run_gradient_descent_feng(X, y, iterations=1000000, alpha = 1e-1)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("Normalized x x**2, x**3 feature")
plt.plot(x,X@model_w + model_b, label="Predicted Value"); plt.xlabel("x"); plt.ylabel("y"); plt.legend(); plt.show()
```

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231123142231991.png" alt="image-20231123142231991" style="zoom:67%;" />



## 总结

1.线性回归可以使用特征工程对复杂甚至高度非线性函数进行建模

2.在进行特征工程时应用特征缩放很重要