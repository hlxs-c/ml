# 逻辑回归的梯度下降

## 逻辑回归的梯度下降的

根据 

[代价函数]: C:\Data\Markdown\MachineLearning\1.监督学习\2.逻辑回归\5.代价函数.md	"代价函数"

中的分析，二分类的逻辑回归的代价函数可以化简为：
$$
J(\mathbf{w},b) = - \frac{1}{m} \sum_{i=1}^m y^{(i)} * log(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) + (1-y^{(i)})*log(1-f_{\mathbf{w},b}(\mathbf{x}^{(i)}))
$$
在得到这一个代价函数以后，我们便可以使用 **梯度下降算法** 来求得使代价函数最小的参数了。

算法为：
$$
\text{Repeat Until Convex} \\
w_j = w_j - \alpha * \frac{\partial J(\mathbf{w},b)}{\partial w_j} \\
b = b - \alpha * \frac{\partial J(\mathbf{w},b)}{\partial b} \\

Note:注意同时（同步）更新所有参数
$$
求导后可得：
$$
\text{Repeat Until Convex} \\
w_j = w_j - \alpha * \frac{1}{m} \sum_{i=1}^m (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)}) \mathbf{x}_j^{(i)} \\
b = b - \alpha * \frac{1}{m}\sum_{i=1}^m (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)})  \\

Note:注意同时（同步）更新所有参数
$$


## 偏导推导过程

将 $f_{\mathbf{w},b}(\mathbf{x}) = \frac{1}{1+e^{-(\mathbf{wx}+b)}}$ 代入代价函数中可得：
$$
J(\mathbf{w},b) = - \frac{1}{m} \sum_{i=1}^m y^{(i)} * log(\frac{1}{1+e^{-(\mathbf{wx^{(i)}}+b)}}) + (1-y^{(i)})*log(1-\frac{1}{1+e^{-(\mathbf{wx^{(i)}}+b)}})) \\
=- \frac{1}{m} \sum_{i=1}^m -y^{(i)}log(1+e^{-(\mathbf{wx^{(i)}}+b)})+(1-y^{(i)})log(\frac{e^{-(\mathbf{wx^{(i)}}+b)}}{1+e^{-(\mathbf{wx^{(i)}}+b)}}) \\
$$
其中：
$$
\frac{e^{-(\mathbf{wx^{(i)}}+b)}}{1+e^{-(\mathbf{wx^{(i)}}+b)}} = \frac{1}{1+e^{\mathbf{wx}+b}} \\\\
左式上下同乘 e^{\mathbf{wx}+b}即可得到右式
$$


则：
$$
J(\mathbf{w},b) = - \frac{1}{m} \sum_{i=1}^m -y^{(i)}log(1+e^{-(\mathbf{wx^{(i)}}+b)})+(1-y^{(i)})log(\frac{1}{1+e^{(\mathbf{wx^{(i)}}+b)}}) \\
=- \frac{1}{m} \sum_{i=1}^m -y^{(i)}log(1+e^{-(\mathbf{wx^{(i)}}+b)}) - (1-y^{(i)})log(1+e^{(\mathbf{wx^{(i)}}+b)})
$$
对代价函数求偏导可得：
$$
\frac{\partial J(\mathbf{w},b)}{\partial w_j} = - \frac{1}{m} \sum_{i=1}^m -y^{(i)} \frac{-\mathbf{x}_{j}^{(i)}e^{-(\mathbf{wx^{(i)}}+b)}}{1+e^{-(\mathbf{wx^{(i)}}+b)}} -(1-y^{(i)}) \frac{\mathbf{x}_{j}^{(i)} e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}}
$$
由于：
$$
\frac{e^{-(\mathbf{wx^{(i)}}+b)}}{1+e^{-(\mathbf{wx^{(i)}}+b)}} = \frac{1}{1+e^{(\mathbf{wx^{(i)}}+b)}} \\ \\
左式上下同乘 e^{\mathbf{wx^{(i)}+b}}即可得到右式
$$
则：
$$
\frac{\partial J(\mathbf{w},b)}{\partial w_j} = - \frac{1}{m} \sum_{i=1}^m -y^{(i)} \frac{-\mathbf{x}_{j}^{(i)}e^{-(\mathbf{wx^{(i)}}+b)}}{1+e^{-(\mathbf{wx^{(i)}}+b)}} -(1-y^{(i)}) \frac{\mathbf{x}_{j}^{(i)} e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} \\
= - \frac{1}{m} \sum_{i=1}^m y^{(i)} \frac{\mathbf{x}_{j}^{(i)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} -(1-y^{(i)}) \frac{\mathbf{x}_{j}^{(i)} e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} \\
= - \frac{1}{m} \sum_{i=1}^m \frac{y^{(i)} \mathbf{x}_{j}^{(i)} - \mathbf{x}_{j}^{(i)} e^{(\mathbf{wx^{(i)}}+b)} + y^{(i)}\mathbf{x}_j^{(i)}e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} \\
=- \frac{1}{m} \sum_{i=1}^m \frac{y^{(i)}(1+e^{(\mathbf{wx^{(i)}}+b)})-e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} * \mathbf{x}_{j}^{(i)} \\
=- \frac{1}{m} \sum_{i=1}^m (y^{(i)}-\frac{e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} )* \mathbf{x}_{j}^{(i)} \\
$$
由于：
$$
\frac{e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} = \frac{1}{1+e^{-(\mathbf{wx^{(i)}}+b)}} \\ \\
左式上下同乘以 e^{-(\mathbf{wx^{(i)}}+b)}得到右式
$$
则：
$$
\frac{\partial J(\mathbf{w},b)}{\partial w_j} = - \frac{1}{m} \sum_{i=1}^m (y^{(i)}-\frac{e^{(\mathbf{wx^{(i)}}+b)}}{1+e^{(\mathbf{wx^{(i)}}+b)}} )* \mathbf{x}_{j}^{(i)} \\
=- \frac{1}{m} \sum_{i=1}^m (y^{(i)}-\frac{1}{1+e^{-(\mathbf{wx^{(i)}}+b)}} )* \mathbf{x}_{j}^{(i)} \\
=\frac{1}{m} \sum_{i=1}^m (\frac{1}{1+e^{-(\mathbf{wx^{(i)}}+b)}}-y^{(i)} )* \mathbf{x}_{j}^{(i)} \\
=\frac{1}{m} \sum_{i=1}^m (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)} )* \mathbf{x}_{j}^{(i)} \\
$$
即最终通过推导可得，**代价函数对参数$w_j$的偏导 $\frac{\partial J(\mathbf{w},b)}{\partial w_j}$** 为：
$$
\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \frac{1}{m} \sum_{i=1}^m (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)} )* \mathbf{x}_{j}^{(i)}
$$
同理，**代价函数对参数 $b$ 的偏导$\frac{\partial J(\mathbf{w},b)}{\partial b}$ **为：
$$
\frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m} \sum_{i=1}^m (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)} )
$$


将上述**偏导**代入逻辑回归的梯度下降算法可得：
$$
\text{Repeat Until Convex} \\
w_j = w_j - \alpha * \frac{1}{m} \sum_{i=1}^m (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)}) \mathbf{x}_j^{(i)} \\
b = b - \alpha * \frac{1}{m}\sum_{i=1}^m (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)})  \\

Note:注意同时（同步）更新所有参数
$$
**注意**：虽然经过推导得到的逻辑回归的梯度下降算法表面上看与线性回归的梯度下降算法相同，但是这里的 $f_{\mathbf{w},b}(\mathbf{x}) = \frac{1}{1+e^{-(\mathbf{wx}+b)}}$ ，与线性回归中的$f_{\mathbf{w},b}(\mathbf{x})$ 是不相同的，所以实际上是不一样的。



<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231201171341946.png" alt="image-20231201171341946" style="zoom:50%;" />

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231201171349358.png" alt="image-20231201171349358" style="zoom:50%;" />



> 虽然逻辑回归与线性回归使用梯度下降算法更新参数的规则看起来基本相同，但由于模型的定义不同，所以逻辑回归的梯度下降算法跟线性回归的梯度下降实际上是两个完全不同的东西。



在线性回归中，谈论了如何监控梯度下降算法以确保其收敛，这通常也可以把同样的方法用在逻辑回归中，用来检测梯度下降，以确保它正常收敛。

同样的，在线性回归中，使用到了特征缩放来提高梯度下降算法的收敛速度，这也适用于逻辑回归。如果特征范围差距很大的话，那么就应该使用特征缩放的方法，以使逻辑回归的梯度下降收敛更快。



