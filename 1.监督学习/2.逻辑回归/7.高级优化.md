# 高级优化



## 另一个角度思考梯度下降

我们有一个代价函数 $J(\mathbf{w},b)$ ，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数 $(\mathbf{w},b)$ 时，代码会计算出两样东西：

- 代价（成本）：$J(\mathbf{w},b)$
- 梯度（偏导项）：$\frac{\partial J(\mathbf{w},b)}{\partial w_j}$ 和 $\frac{\partial J(\mathbf{w},b)}{\partial b}$

![img](http://www.ai-start.com/ml2014/images/394a1d763425c4ecf12f8f98a392067f.png)

假设我们已经完成了可以计算 **代价** 和 **梯度** 的代码，那么梯度下降算法做的就是反复执行这些 **参数更新** 的步骤。

另一种考虑梯度下降的思路是：我们需要编写代码来计算 **代价$J(\mathbf{w},b)$**和 **梯度$\frac{\partial J(\mathbf{w},b)}{\partial w_j}$**，然后把这些代入到梯度下降中，然后它就可以为我们最小化代价函数 $J(\mathbf{w},b)$。

对于梯度下降来说，从技术上来讲，实际上并不需要编写代码来计算 **代价$J(\mathbf{w},b)$**，而只需要编写 代码来计算 **梯度$\frac{\partial J(\mathbf{w},b)}{\partial w_j}$**，但是，如果我们希望代码还要能够监控代价函数 $J(\mathbf{w},b)$的收敛性，则我们就需要编写代码来计算 **代价$J(\mathbf{w},b)$**和 **梯度$\frac{\partial J(\mathbf{w},b)}{\partial w_j}$。

所以，在编写完能够计算 **代价**和**梯度**的代码之后，我们就可以使用梯度下降。



## 其他优化算法

然而，**梯度下降** 并不是我们可以使用的唯一算法，还有一些其他的算法，更高级、更复杂的算法：

- 共轭梯度法 `BFGS`（变尺度法）
- `L-BFGS`（限制尺度法）

这两种方法需要一种方法来计算 **代价$J(\mathbf{w},b)$**和 **梯度$\frac{\partial J(\mathbf{w},b)}{\partial w_j}$**，然后使用比 **梯度下降** 共呢个复杂的算法来最小化代价函数。

这些算法有许多优点：

- 使用这其中任何一个算法，通常不需要我们手动选择学习率 $\alpha$，所以对于这些算法的一种思路是：给出计算 **代价$J(\mathbf{w},b)$**和 **梯度$\frac{\partial J(\mathbf{w},b)}{\partial w_j}$**的方法。我们可以认为算法有一个智能的内部循环，而且，事实上，它们确实有一个智能的内部循环，称为**线性搜索（linear search)**算法，它可以自动尝试不同的学习率 $\alpha$，并自动选择一个好的学习率$\alpha$，因此它甚至可以为每次迭代选择不同的学习速率，那么我们就不需要自己选择。
- 这些算法实际上在做更复杂的事情，不仅仅是选择一个好的学习速率，所以它们往往最终比梯度下降收敛得块多了。

