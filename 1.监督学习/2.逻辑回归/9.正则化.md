# 正则化



## 1.过拟合的问题

**线性回归** 和 **逻辑回归** 学习算法能够有效解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到 **过拟合（`over-fiting`）**的问题，可能会导致它们效果很差。

**过拟合**：如果我们有非常多的问题，我们通过学习得到的模型可能能够非常好适应训练集（代价函数可能几乎为 $0$），但是可能会不能推广到新的数据上。



## 线性回归中的过拟合

![image-20231202141632169](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231202141632169.png)

如上图所示：

- 第一个模型是一个 **一次函数**模型，**欠拟合**，不能很好的拟合训练集
- 第三个模型是一个 **四次函数**模型，**过拟合**，过于强调拟合原始数据（训练集），而丢失了算法的本质——预测新数据；
  - 可以看到，如果给出一个新的数据输入使第三个模型预测，它将表现的很差。
  - 这是**过拟合**，虽然能够非常好地适应训练集，但是在新输入的变量上进行预测时，效果不会好。



### 逻辑回归（分类问题）中的过拟合

![image-20231202142010961](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231202142010961.png)



### 处理过拟合的方法

1. 丢弃一些不能帮助我们能够正确预测的特征：
   1. 手工选择保留哪些特征
   2. 使用一些模型选择的算法来帮忙选择（例如`PCA`）
   3. 缺点：舍弃一部分特征也意味着舍弃了关于问题的一些信息。（例如，可能所有的特征对问题的预测都是有效的，则此时舍弃某些特征将会丢失有用的信息）
2. 正则化
   1. 保留所有的特征，但是减少参数的大小（`magnitude`）
   2. 优点：当我们有很多的特征且每一个特征都能够对预测的$y$ 值产生一些影响，这个方法非常有效





## 2.正则化代价函数

### 2.1 从线性回归过拟合引入正则化的思想

![image-20231202154107584](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231202154107584.png)

在上图中的回归问题中，模型 $f_{\mathbf{w},b}(\mathbf{x})=w_1x+w_2x^2+w_3x^3+w_4x^4+b$ 中的高次项导致了过拟合的发生，所以如果我们能够让这些高次项的系数（权重/参数）接近于0的话，那么我们就能解决过拟合的问题。所以我们要做的就是**在一定程度上减小这些参数的值，这就是正则化的基本方法**。

我们决定要减少参数 $w_3,w_4$ 的大小，则要做的便是修改代价函数，在其中对$w_3,w_4$ 设置惩罚项。这样做的话，我们在尝试最小化代价函数时也就需要将这些惩罚纳入考虑中，并最终导致选择较小的$w_3,w_4$。修改后的代价函数可能如下：
$$
J(f_{\mathbf{w},b}(\mathbf{x}) = \frac{1}{2m} \left[ \sum_{i=1}^m(f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)})^2 \space + \space +1000w_3^2 + 1000w_4^2 \right])
\\ \\
其中1000只是随便一个比较大的数
$$
假如我们要最小化这个代价函数，要使这个代价函数尽可能小的方法只有一个：就是 $w_3,w_4$ 要尽可能得小，这是因为在上述代价函数中 $w_3,w_4$ 的惩罚系数为1000很大，它会使整个函数的值变得很大。

所以**如果我们要最小化这个代价函数，就需要使 $w_3,w_4$ 尽可能得接近于0。这就相当于我们直接去掉了模型中的 $w_3x^3$ 和 $w_4x^4$ 这两项一样**。从而，通过这样的代价函数选择出的 $w_3,w_4$ 对模型预测的影响就比之前要小许多，从而避免过拟合的发生。



### 2.2 归纳正则化修改代价函数的方法

假如我们有非常多的特征，而我们并不知道其中哪些特征需要惩罚，则我们将对所有的特征进行惩罚，并让代价函数最优化的算法来选择这些惩罚的程度。

添加正则化项后的代价函数如下所示：
$$
J(\mathbf{w},b)=\frac{1}{2m} \left[\sum_{i=1}^{m}(f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)})^2 + \lambda \sum_{j=1}^n w_j^2 \right]
$$
其中的 $\lambda$ 被称为 **正则化参数**（Regularization Parameter）。

使用上述代价函数进行最优化，得到 `经过正则化处理的模型` 与 `未经过正则化的原模型` 的可能对比如下图所示：

![img](http://www.ai-start.com/ml2014/images/ea76cc5394cf298f2414f230bcded0bd.jpg)

**那为什么增加的正则化项 $\lambda \sum_{j=1}^n w_j^2$ 可以避免过拟合的问题呢？**—— **$\lambda$ 可以控制两个不同目标之间的取舍。**

![img](https://pic2.zhimg.com/80/v2-168883c9d86dd25a26fff5b7f33cea55_720w.webp)

- 目标1：更好地拟合数据集
- 目标2：使参数变小，从而使得模型变得更”简单“、”平滑“，避免过拟合
  - 为什么增加的正则化项 $\lambda \sum_{j=1}^n w_j^2$ 可以使参数 $w_j$ 的值变小——因为在添加了该项之后，为了使代价函数尽可能地小，这一项的值应该尽可能得小，从而会使得所有参数$w_j$的值都会在一定程度上减小。

**两者相互平衡，从而达到一种相互制约的关系，最终找到一个平衡点，从而更好地拟合数据集并且具有良好的泛化能力**。



### 2.3 正则化参数过大的问题

如果选择的**正则化参数 $\lambda$** 过大，则会把所有的参数都最小化，导致模型可能变成 $f_{\mathbf{w},b}=b$，也就是如上图中红色直线所示的情况，造成欠拟合。

这是因为，如果我们令正则化参数 $\lambda$ 过大，则为了使代价函数的值尽可能得小，则正则化项 $\lambda \sum_{j=1}^n w_j^2$ 中的 $\sum_{j=1}^n w_j^2$ 就需要尽可能地接近于0，否则由于正则化参数$\lambda$ 太大，正则化项$\lambda \sum_{j=1}^n w_j^2$的值会很大，从而使得代价函数的值很大。

而要使 $\sum_{j=1}^n w_j^2$ 尽可能地趋近于0，则其中所有的参数 $w_j$ 都应该趋近于0，则也就意味着模型中这些参数对应的项几乎被去掉了，从而模型变成 $f_{\mathbf{w},b}=b$。

所以对于正则化，我们要选择一个合理的正则化参数 $\lambda$的值，这样才能更好地应用正则化。



