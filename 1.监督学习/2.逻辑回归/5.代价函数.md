# 代价函数

## 1. 逻辑回归代价函数的定义

对于线性回归模型，我们定义的代价函数是所有模型的误差的平方和（`squared err `）。理论上来说，我们也可以对逻辑回归模型沿用这个代价函数，但是问题在于，当我们将逻辑回归模型：
$$
f_{\mathbf{w},b}(\mathbf{x})=\frac{1}{1+e^{\mathbf{w} \mathbf{x}+b}}
$$
代入到平方误差函数中，由于$f_{\mathbf{w},b}$ 是一个非线性函数，则代价函数将会是一个非凸函数（**`non-convexfunction`**），这意味着代价函数将会有许多局部最小值，这将影响梯度下降算法寻找全局最小值，如下图所示：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231129151519366.png" alt="image-20231129151519366" style="zoom:33%;" />



线性回归的代价函数为（平方误差函数）：
$$
J(\mathbf{w},b) = \frac{1}{2m}\sum_{i=1}^m(f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)})^2 \\ 
=\frac{1}{m}\sum_{i=1}^m \frac{1}{2} (f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)})^2
$$
将其中的项 $\frac{1}{2} (f_{\mathbf{w},b}(x^{(i)})-y^{(i)})^2$ 抽取为 $Cost(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)})$ 或 $L(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)})$，即代价函数变为：
$$
J(\mathbf{w},b) = \frac{1}{m} \sum_{i=1}^m Cost(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)}) \\ 或 \\
J(\mathbf{w},b) = \frac{1}{m} \sum_{i=1}^m L(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)})
$$

**$Cost(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)})$ 的含义**：在输出的预测值为$f_{\mathbf{w},b(\mathbf{x}^{(i)})}$，而真实标签为$y^{(i)}$时，学习算法应该付出的代价。



重新定义逻辑回归的代价函数为：
$$
J(\mathbf{w},b) = \frac{1}{m} \sum_{i=1}^m Cost(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)}) \\ 或 \\
J(\mathbf{w},b) = \frac{1}{m} \sum_{i=1}^m L(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)})
$$
其中：
$$
Cost(f_{\mathbf{w},b}(\mathbf{x}),y)=
\begin{cases}
-log(f_{\mathbf{w},b}(\mathbf{x})) & if \space y=1 \\
-log(1-f_{\mathbf{w},b}(\mathbf{x})) & if \space y=0 

\end{cases}
\\ 或 \\
L(f_{\mathbf{w},b}(\mathbf{x}),y)=
\begin{cases}
-log(f_{\mathbf{w},b}(\mathbf{x})) & if \space y=1 \\
-log(1-f_{\mathbf{w},b}(\mathbf{x})) & if \space y=0 

\end{cases}
$$
分析该代价函数：

- 当 $y=1$时，即真实标签（即样本的真实类别）为 $1$ 时，$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)=-log(f_{\mathbf{w},b}(\mathbf{x}))$
  - <img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231129150055563.png" alt="image-20231129150055563" style="zoom: 33%;" />
  - 分析上图中左图可知，**在真实标签为 $1$ 时**：
    - 当$f_{\mathbf{w},b}(\mathbf{x})$的输出越接近 $1$，则代价项$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)$ 或者 $L(f_{\mathbf{w},b}(\mathbf{x}),y)$ 就越接近$0$，在$f_{\mathbf{w},b}(\mathbf{x})$的输出为1时，代价项的值为0；
    - 当$f_{\mathbf{w},b}(\mathbf{x})$的输出越接近 $0$，则代价项$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)$ 或者 $L(f_{\mathbf{w},b}(\mathbf{x}),y)$ 就越接近正无穷
    - 这是合理的，因为当真实标签为 $1$时，如果预测值越接近$1$ ，则意味着算法正确性越高，则算法应该付出的代价应该越小（即越接近$0$）；而当真实标签为$1$，而预测值越接近$0$，则意味着算法正确性越低，则算法应该付出的代价应该越大（即越接近正无穷）；
- 当 $y=0$时，即真实标签（即样本的真实类别）为 $0$ 时，$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)=-log(1-f_{\mathbf{w},b}(\mathbf{x}))$
  - <img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231129150946671.png" alt="image-20231129150946671" style="zoom:33%;" />
  - 分析上图中左图可知，**在真实标签为 $0$ 时**：
    - 当$f_{\mathbf{w},b}(\mathbf{x})$的输出越接近 $0$，则代价项$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)$ 或者 $L(f_{\mathbf{w},b}(\mathbf{x}),y)$ 就越接近 $0$，在$f_{\mathbf{w},b}(\mathbf{x})$的输出为 $0$ 时，代价项的值为 $0$；
    - 当$f_{\mathbf{w},b}(\mathbf{x})$的输出越接近 $1$，则代价项$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)$ 或者 $L(f_{\mathbf{w},b}(\mathbf{x}),y)$ 就越接近正无穷
    - 这是合理的，因为当真实标签为 $0$ 时，如果预测值越接近 $0$，则意味着算法正确性越高，则算法应该付出的代价应该越小（即越接近0）；而当真实标签为 $0$，而预测值越接近 $1$，则意味着算法正确性越低，则算法应该付出的代价应该越大（即越接近正无穷）； 

> 这样构建的$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)$ 函数的特点是：
>
> - 当实际的 $y=1$ 且 $f_{\mathbf{w},b}(\mathbf{x})$ 也为 $1$ 时误差为0；当 $y=1$ 但 $f_{\mathbf{w},b}(\mathbf{x})$ 不为1时，误差随着$f_{\mathbf{w},b}(\mathbf{x})$ 变小而变大；
> - 当实际的 $y=0$ 且 $f_{\mathbf{w},b}(\mathbf{x})$ 也为 $0$ 时误差为0；当 $y=0$ 但 $f_{\mathbf{w},b}(\mathbf{x})$ 不为0时，误差随着 $f_{\mathbf{w},b}(\mathbf{x})$ 变大而变大





## 2. 逻辑回归代价函数的简化（二元分类）

由于 **二元分类** 问题中，样本的真是标签只有两种，即 $y ∈ {0,1}$，故可以将 代价项$Cost(f_{\mathbf{w},b}(\mathbf{x}),y)$ 进行简化：

原始代价项 $Cost(f_{\mathbf{w},b}(\mathbf{x}),y)$ 为：
$$
Cost(f_{\mathbf{w},b}(\mathbf{x}),y)=
\begin{cases}
-log(f_{\mathbf{w},b}(\mathbf{x})) & if \space y=1 \\
-log(1-f_{\mathbf{w},b}(\mathbf{x})) & if \space y=0 

\end{cases}
\\ 或 \\
L(f_{\mathbf{w},b}(\mathbf{x}),y)=
\begin{cases}
-log(f_{\mathbf{w},b}(\mathbf{x})) & if \space y=1 \\
-log(1-f_{\mathbf{w},b}(\mathbf{x})) & if \space y=0 

\end{cases}
$$
简化（利用 $y ∈ {0,1}$ 的特性简化）：
$$
Cost(f_{\mathbf{w},b}(\mathbf{x}),y) = -y*log(f_{\mathbf{w},b}(\mathbf{x}) - (1-y)*log(1-f_{\mathbf{w},b}(\mathbf{x})) \\
或  \\
L(f_{\mathbf{w},b}(\mathbf{x}),y) = -y*log(f_{\mathbf{w},b}(\mathbf{x}) - (1-y)*log(1-f_{\mathbf{w},b}(\mathbf{x}))
$$
将代价项代入到 **代价函数**中，可得：
$$
J(\mathbf{w},b) = \frac{1}{m} \sum_{i=1}^{m}Cost(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),y^{(i)}) \\
=\frac{1}{m} \sum_{i=1}^m -y^{(i)} * log(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - (1-y^{(i)})*log(1-f_{\mathbf{w},b}(\mathbf{x}^{(i)})) \\
=- \frac{1}{m} \sum_{i=1}^m y^{(i)} * log(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) + (1-y^{(i)})*log(1-f_{\mathbf{w},b}(\mathbf{x}^{(i)}))
$$



## 3. 逻辑回归代价函数的实现

### 3.1 循环版本

```python
def compute_cost_logistic(X, y, w, b):
    """
    Computes cost using logistic loss, non-matirx version
    Args:
    	X (ndarray(m,n)): matrix of examples with a features
    	y (ndarray(m,)) : target values
    	w (ndarray(n,)) : parameters of model
    	b (scalar)      : parameter of model
    
    Returns:
    	cost (scalar)   : cost
    """
    
    m, n = X.shape
    cost = 0.0
    for i in range(m):
        z_i = np.dot(X[i], w) + b
        f_wb_i = sigmoid(z_i)	# 计算单个样本的预测
        cost += -y[i] * np.log(f_wb_i) - (1-y[i]) * np.log(1-f_wb_i)	# 计算单个样本的预测误差
   	cost = cost/m
    
    return cost
```





## 4. 避免数值计算溢出

### 1.数学分析

通过上述分析，在二元分类中，逻辑回归的**代价函数**为：
$$
J(\mathbf{w},b) = - \frac{1}{m} \sum_{i=1}^m y^{(i)} * log(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) + (1-y^{(i)})*log(1-f_{\mathbf{w},b}(\mathbf{x}^{(i)}))
$$
**注意**到：当逻辑回归模型的输出 $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ 非常接近于 $0$ 或 $1$ 时， 则会出现 $log(f_{\mathbf{w},b}(\mathbf{x}^{(i)}))$ 或 $log(1-f_{\mathbf{w},b}(\mathbf{x}^{(i)}))$ 是在计算类似于 $log(0)$ 的值，则由于 $log(0) = -inf$（即$log(0)$的值接近负无穷），此时会出现数值计算溢出的情况。

为了避免数值计算溢出，我们可以变换上述代价函数的形式：

- 令 $z = \mathbf{wx} + b$
- 则 $f_{\mathbf{w},b}(\mathbf{x}) = sigmoid(z)$，其中$sigmoid(z) = \frac{1}{1+e^{-z}}$



$f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ 代表预测输出 $y=1$ 的概率，则 $1-f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ 为预测输出 $y=0$ 的概率，也即：

- $y=1$ 的概率为：$P(y=1|\mathbf{x}^{(i)};\mathbf{w},b) = \frac{1}{1+e^{-z}}$
- $y=0$ 的概率为：$P(y=1|\mathbf{x}^{(i)};\mathbf{w},b) = 1-\frac{1}{1+e^{-z}} = \frac{e^{-z}}{1+e^{-z}}$

将此代入逻辑回归的代价项$L(f_{\mathbf{w},b}(\mathbf{x}),y)$：
$$
L(f_{\mathbf{w},b}(\mathbf{x}),y) = -y*log(f_{\mathbf{w},b}(\mathbf{x}) - (1-y)*log(1-f_{\mathbf{w},b}(\mathbf{x})) \\
=-y*log(\frac{1}{1+e^{-z}}) - (1-y)*log(1-\frac{1}{1+e^{-z}}) \\
=-y*log(\frac{1}{1+e^{-z}}) - (1-y)*log(\frac{e^{-z}}{1+e^{-z}})\\
=-y*log(\frac{1}{1+e^{-z}}) + y*log(\frac{e^{-z}}{1+e^{-z}}) - log(\frac{e^{-z}}{1+e^{-z}})\\
=-y*\left[log(\frac{1}{1+e^{-z}})-log(\frac{e^{-z}}{1+e^{-z}})\right] - log(\frac{1}{1+e^{z}})\\
=-y*log(\frac{1}{e^{-z}})+log(1+e^{z}) \\
=-y*log(e^z)+log(1+e^z) & 此处利用log=ln等价替换\\ 
=-y*z+log(1+e^z)
$$
此时，由于 $e^z > 0$，故对数函数的输入$(1+e^z)>1$，就不再会出现 $log(0)$的情况了，从而避免计算$log(0)$ 的数值计算溢出。

**可以避免数值计算溢出的二元分类逻辑回归的代价项（即某个样本的预测的误差）**：
$$
L(f_{\mathbf{w},b}(\mathbf{x}),y) = =-y*z+log(1+e^z)
$$


但是注意到：$z=\mathbf{wx}+b$，$z$ 可能是一个非常大的实数，所以可能会出现计算非常大的 $e^z$，从而又有可能出现数值计算溢出的情况，为了避免这种数值计算溢出的情况，则可以利用 **近似计算** 的方法来计算 $log(1+e^z)$ ：

- 令 $ h(x) = log(1+e^x)$

则利用 **近似计算** 的方法来计算 $h(x)$为：
$$
h(x) = 
\begin{cases}
log(1+e^x) & x \le c \\
x 		   & x \gt c \\
\end{cases}
$$

> 因为当 $x$ 非常大时，$e^x$ 的值非常大，可能会出现数值溢出的情况，而当 $e^x$ 非常大时，则此时可以将 $(1+e^x)$ 中的 $1$ 忽略掉，从而 $h(x)=log(e^x)$，再利用 $log = ln$ 的等价替换，则 $h(x) = x$，从而实现了对 $h(x)$的**近似计算**，避免了计算 $e^x$ 时的数值溢出。

- 也就是说，当 $x$ 非常大时，我们将 $h(x)$ 近似认为是一个 恒等函数 $h(x)=x$
- 其中，选择 $c=20$ 是一个非常典型的选择，但是根据浮点精度，可能需要选择一个更大或更小的数



参考链接：[实践中交叉熵的数值计算](https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice)



### 2.代码实现

#### 2.1 实现对 $log(1+e^x)$ 的近似计算

```python
def log_lpexp(x, maximum=20):
    """
    实现对 log(1+e^x)的近似计算，原因如上所述，也可参考https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice
    
    参数：
    	x (ndarray (n,1) or (n,)) : input
    返回值：
    	out (ndarray shape like x) : 是对 log(1+e^x)的近似计算的值
    """
    
    out = np.zeros_like(x, dtype=float)
    i = x <= maximum
    ni = np.logical_not(i)
    
    out[i] = np.log(1+np.exp[x[i]])
    out[ni] = x[ni]
    return out
```

> The function first initializes an output array `out` with the same shape as `x` and data type as `float`.
>
> Next, it creates two boolean masks:
>
> - `i`: A mask for elements in `x` that are less than or equal to `maximum`.
> - `ni`: A mask for elements in `x` that are greater than `maximum`.
>
> Then, it applies the approximation for `log(1+exp(x))` to the elements in `x` that satisfy the condition `x <= maximum`. It uses the `np.log` and `np.exp` functions to compute the approximation.
>
> For the elements in `x` that exceed the `maximum` value, it assigns the corresponding elements from `x` directly to `out`.
>
> Finally, it returns the computed `out` array.
>
> This function allows you to compute the approximation for `log(1+exp(x))` efficiently, avoiding overflow issues when dealing with large values of `x`.

> 在 `log_lpexp()`中，`i` 是一个布尔类型的数组，它用作索引来选择满足条件 $ x \le maximum$ 的元素。
>
> 布尔类型的数组可以用作索引，其中`true` 值对应于选择的元素，而`false`值对应于不选择的元素。
>
> ```python
> out[i]  = np.log(1 + np.exp(x[i]))
> ```
>
> 在该代码中，`i` 中`true`值所在的索引用于选择`out`和`x`中的元素，例如， $i=[true, false, true, false, false]$，则上述代码代表的意思就是：
>
> ```python
> out[0] = np.log(1+np.exp(x[0]))
> out[2] = np.log(1+np.exp(x[2]))
> # 而其他元素则不被选择计算
> ```
>
> 



#### 2.2 实现避免数值计算溢出的二元分类逻辑回归的代价函数（循环版本）

```python
def compute_cost_logistic(X, y, w, b):
    """
    Compute cost using logistic loss, non-matrix version (safe)
    
    Args:
    	X (ndarray(m,n)): matrix of examples with a features
    	y (ndarray(m,)) : target values
    	w (ndarray(n,)) : parameters of model
    	b (scalar)      : parameter of model
    
    Returns:
    	cost (scalar)   : cost
    """
    
    m, n = X.shape
    cost = 0.0
    for i in range(m):
        # 计算样本X[i]对应的 z = wx+b
        z_i = np.dot(X[i], w) + b
        # 计算样本X[i]的预测的交叉熵损失
        cost += -(y[i]*z_i) + log_lpexp(z_i)
	cost = cost/m
    
    return cost
```







#### 2.3 将避免数值计算溢出和普通版本结合在一起

```python
def compute_cost_logistic(X, y, w, b, safe=False):
    """
    Computes cost using logistic loss, non-matirx version
    Args:
    	X (ndarray(m,n)): matrix of examples with a features
    	y (ndarray(m,)) : target values
    	w (ndarray(n,)) : parameters of model
    	b (scalar)      : parameter of model
    	safe (boolean)  : if safe=true, the select under/overflow safe algorithm
    		使用safe控制选择哪一个版本的损失函数
    
    Returns:
    	cost (scalar)   : cost
    """
    
    m, n = X.shape
    cost = 0.0
    for i in range(m):
        z_i = np.dot(X[i], w) + b
        if safe:	# avoids overflows
            cost += -(y[i] * z_i) + log_lpexp(z_i)
        else:
            f_wb_i = sigmoid(z_i)
            cost += -y[i] * np.log(f_wb_i) - (1-y[i]) * np.log(1-f_wb_i)
  	cost = cost/m
    return cost
```



## 5. 逻辑回归代价函数的实现（向量化版本）

```python
def compute_cost_logistic_matrix(X, y, w, b, lambda_=0, safe=True):
    """
    Computes the cost using matrices
    Args:
        X (ndarray (m,n)) : matrix of examples
        y (ndarray (m,))  : target value of each example
        w (ndarray (n,))  : values of parameters of the model
        b (scalar)        : value of parameter of the model
        lambda_ (scalar)  : value of regularization parameter
        safe (boolean)    : if safe=true, then select under/overflow safe algorithm
    Returns:
        cost (scalar)     : cost
    """
    
    m = X.shape[0]
    y = y.reshape(-1, 1) # 保证是二维的
    w = w.reshape(-1, 1) # 保证是二维的
    if safe:  # safe from overflow
        z = X @ w + b
        cost = -(y * z) + log_lpexp(z)
        cost = np.sum(cost)/m
    else:
        f = sigmoid(X @ w + b)
        cost = (1/m) * (np.dot(-y.T, np.log(f))) - np.dot((1-y).T, np.log(1-f))
        cost = cost[0,0]
    
    reg_cost = (lambda_/(2*m)) * np.sum(w**2)
    
    total_cost = cost + reg_cost
    
    return total_cost
```

