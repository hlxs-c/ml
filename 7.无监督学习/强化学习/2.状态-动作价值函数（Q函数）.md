# 状态-动作价值函数（Q函数）

## 1.举例说明

![image-20240116190814686](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116190814686.png)

状态-动作价值函数是关于所处的状态以及在该状态下选择的动作的函数：

**Q(s,a)** = **回报：**

- 从状态`s`出发
- 采取一次动作`a`
- 之后以最佳决策行动

即`Q(s,a)`代表从状态`s`出发，采取一次动作`a`，然后以最佳策略行动带来的回报。

例如，在火星车的例子中，最佳决策如下图所示：

![image-20240116184124153](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116184124153.png)

**Q(2, 右)** = 

- 从状态2出发
- 先向右走一步，走到状态3
- 然后从状态3开始以最佳决策行动，即向左、向左

则在这个过程中的奖励序列为：0（初始）、0、0、100，回报为（假设折扣因子为0.5）：
$$
回报=0+(0.5) * 0 + (0.5)^2 * 0 + (0.5)^3 * 100 = 12.5
$$
故，**Q(2,右) = 12.5**。

同理有：Q(2,左) = 0 + $(0.5)$*100 = 50，利用这种方式计算出每个状态的Q(s,左)和Q(s,右)，并将其标记到各个状态的左右，则得到如下图：

![image-20240116184723050](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116184723050.png)

通过对比计算了每个状态的Q(s,左)和Q(s,右)的上图 以及 最佳策略的图，可以发现一件有趣的事情：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116185030057.png" alt="image-20240116185030057" style="zoom:80%;" />

在状态`s`下：

- 如果$Q(s, 左) > Q(s, 右)$，则在最佳决策下该状态的决策也是 **向左**；
- 如果$Q(s, 右) > Q(s, 左)$，则在最佳决策下该状态的决策也是 **向右**；

即从任何状态`s`获得的最佳可能回报是该状态下所有动作$a$下的Q函数的最大值——$max_a Q(s,a)$，在状态`s`下最佳的动作决策是$max_a Q(s,a)$ 对应的动作$a$。 

![image-20240116185545389](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116185545389.png)



从上述分析来看，可知**为什么我们需要计算 `Q(s,a)`：**

- 如果我们有办法计算对于每个状态和每个动作的 `Q(s,a)`，则当我们处于某个状态时，我们需要做的就是遍历所有的动作$a$，并选择动作$a$以使该状态的 `Q(s,a)`的值最大化，从而$\pi(s) = a$



## 2. 贝尔曼方程

从上述对 `状态-动作价值函数` 的分析来看，如果我们可以计算 `Q(s,a)`，则它为我们提供了在每一个状态下挑选出最佳动作决策的方法——选择能够提供最大的`Q(s,a)`值的动作$a$。

在强化学习中，有一个关键方程叫做 **贝尔曼方程**，它可以帮助我们计算 `Q(s,a)`。



首先，确定我们有以下定义：

- 状态-动作价值函数：

![image-20240116190843038](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116190843038.png)

- $s$：代表当前状态
- $a$：代表当前的动作
- $R(s)$：代表当前状态的奖励
- $s^{'}$：代表从状态$s$采取动作 $a$ 之后到达的状态
- $a^{'}$：代表状态$s^{'}$ 可能采取的动作

则 **贝尔曼方程** 为：
$$
Q(s,a) = R(s) + r * max_{a^{'}}Q(s^{'},a^{'})
$$
 **注意**：当 $s$ 是终端状态时，由于其没有下一步动作，也即没有下一个状态，故对于终端状态来说，其贝尔曼方程为：
$$
Q(s_{terminal},a) = R(s)
$$
即终端状态$s_{terminal}$的$Q(s,a)$值始终为该状态下对应的奖励值。

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116192021248.png" alt="image-20240116192021248" style="zoom:50%;" />

**贝尔曼方程** 中的关键概念：

- $R(s)$：代表 **立即奖励**，即该状态下立刻就能够获得的奖励
- $max_{a^{'}}Q(s^{'},a^{'})$：代表从状态$s^{'}$ 出发以最佳策略行动可以获得的最佳回报



从**Q(s,a)** 的定义出发推导**贝尔曼方程**：

- 因为**Q(s,a)** 代表的是从状态$s$ 出发，采取一次动作$a$ 到达状态$s^{'}$ ，然后以最优策略行动获得的最佳回报，表示为方程为：

  - $$
    Q(s,a) = R_1 + r*R_2 + r^2 * R_3 + r^3 * R_4 + ...
    $$

  - 其中，$R_1$ 代表在状态$s$ 下可以立即获得的奖励，即 $R(s)$

  - 而 $r*R_2 + r^2 * R_3 + r^3 * R_4 + ...$ 代表的是以最优策略行动过程中的奖励序列（直到终端状态），并由**折扣因子** 进行了加权和计算

- $$
  Q(s,a) = R_1 + r*R_2 + r^2 * R_3 + r^3 * R_4 + ... \\
  =R(s) + r(R_2 + r * R_3 + r^2 * R_4 + ...) \\
  $$

  - **注意到：**$R_2 + r * R_3 + r^2 * R_4 + ...$ 即是 $Q(s^{'},a^{'})$ 定义，因为这是以最优策略从状态$s^{'}$开始行动获得的回报；

  - 将其代入，则可以得到：

    - $$
      Q(s,a) = R(s) + r * max_{a^{'}}Q(s^{'},a^{'})
      $$

 

## 3. 随机环境

在随机环境中，智能体可能并不能完全按照决策进行行动，则不同的尝试可能得到随机的行动序列，也即可能得到随机的奖励序列。例如，在上述的火星车的例子中，在决策运动方向时，由于随机环境的缘故，可能有10%的概率会往相反的方向行动，则在相同的策略下，不同的尝试可能会得到不同的行动序列（奖励序列）。

即当强化学习问题是随机的时，在相同的策略下，在不同的尝试下我们可能不只会看到一个奖励序列，而是会看到一系列不同的奖励序列。所以，为了处理这种随机问题，我们应该最求的目标不应该是最大化回报，因为那是一个随机数，而是应该最大化 `折扣奖励的总和的平均值`：
$$
Return = Average(R1+r*R_2 + r^2 * R_3 + ...) \\ 
=E(R1+r*R_2 + r^2 * R_3 + ...)
$$
就**平均值**而言：如果采用我们的策略并尝试一千次或十万次或一百万次，我们会得到很多不同的奖励序列，对这些所有不同序列的折扣奖励的总和求平均值，就是我们的回报的**预期**值。

所以，**当强化学习的问题是随机的时，其工作是选择策略 $\pi$ 来最大化折扣奖励的平均值或预期总和。**

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116195811264.png" alt="image-20240116195811264" style="zoom:50%;" />

总而言之，当遇到随机强化学习问题或随机马尔可夫决策过程时，目标是选择一个策略来告诉我们在状态$s$采取什么行动以**最大化预期回报**。

但除了修改回报的公式之外，还需要稍微修改贝尔曼方程，因为现在在状态$s$采取行动$a$时，到达的下一个状态$s^{'}$ 是不确定的（随机的），而贝尔曼方程中的第二项是由到达的下一个状态 $s^{'}$ 决定的，所以为了处理随机的情况，也应该对其进行求预期（平均值）：
$$
Q(s,a) = R(s) + r * E\left[ max_{a^{'}}Q(s^{'},a^{'}) \right]
$$
