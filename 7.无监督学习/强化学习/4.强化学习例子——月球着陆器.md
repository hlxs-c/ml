# 强化学习例子——月球着陆器

## 1. 问题描述

在该强化学习问题中，我们将指挥正在快速接近月球表面的月球着陆器，我们的工作是在适当的时候启动火力推进器，将其安全着陆在月球表面。

- 成功着陆的月球着陆器：成功着陆在两个黄色旗帜之间
  - <img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123174753238.png" alt="image-20240123174753238" style="zoom:67%;" />
- 着陆失败的月球着陆器：
  - <img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123174831110.png" alt="image-20240123174831110" style="zoom:67%;" />

#### 动作

在月球着陆器下降的过程中，我们有4种可能的动作，即在每个状态，我们都有4个可能的**动作**供选择：

- 什么都不做
- 启动左侧推进器
- 启动右侧推进器
- 启动主推进器

#### 状态

月球着陆器的**状态**：

- $x$：`x`轴上的位置
- $y$：`y`轴上的位置
- $v_x$：`x`轴上的速度
- $v_y$：`y`轴上的速度
- $\theta$：月球着陆器向左倾斜或向右倾斜的角度
- $v_{\theta}$：月球着陆器倾斜的角速度
- $l$：月球着陆器左侧是否安全着陆
- $r$：月球着陆器右侧是否安全着陆
  - 其中，$l，r$ 是二进制变量，只有0和1两个取值

故：
$$
state = 
\begin{bmatrix}
x \\
y \\
v_x \\
v_y \\
\theta \\
v_{\theta} \\
l \\
r \\
\end{bmatrix}
$$


#### 奖励函数

![image-20240123185956018](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123185956018.png)



所以 月球着陆器问题 如下：

![image-20240123190154930](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123190154930.png)

我们的目标是学习 **策略$\pi$** ，当给定此处缩写的 **状态s**时，选择一个等于 $a=\pi(s)$ 的**动作a**，从而最大化返还**折扣奖励**的**总和**，其中 **折扣系数$\gamma=0.95$**。





## 2.使用基于深度学习的强化学习来解决问题

使用深度学习解决强化学习问题的关键思想是我们要训练一个神经网络来计算或近似 **状态-动作价值函数$Q(s,a)$**，然后让我们选择好的**动作**。



### 2.1 使用神经网络计算 $Q(s,a)$

学习算法的核心是我们将训练一个神经网络，该网络输入当前状态和当前动作并计算或近似 $Q(s,a)$。

![image-20240123191632989](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123191632989.png)

在该问题中，我们将 $(s,a)$结合起来作为神经网络的输入向量：
$$
input = 
\begin{bmatrix}
s \\
\\ 
a \\
\end{bmatrix}
$$
其中，我们对**动作a**使用 **独热编码** 形成一个向量（因为动作a有4个选择：`nothing、left、main、right`）：
$$
a = 
\begin{bmatrix}
1 \\
0 \\
0 \\
0 \\
\end{bmatrix}
\\
以上动作向量a表示什么都不做
$$
故`input`为：
$$
input = 
\begin{bmatrix}
s \\
\\ 
a \\
\end{bmatrix}
= 
\begin{bmatrix}
x \\
y \\
v_x \\
v_y \\
\theta \\
v_{\theta} \\
l \\
r \\
1 \\
0 \\
0 \\
0 \\
\end{bmatrix}
$$
我们将上述的**输入** 提供给神经网络：

- 第一个隐藏层有64个单元
- 第二个隐藏层有64个单元
- 输出层有1个单元

最终神经网络给出**输出** $Q(s,a)$。 （$Q(s,a)$ 也是训练时的目标值）



如果我们可以在训练以上的神经网络，得到适当的参数，以提供对 $Q(s,a)$ 的良好估计，那么每当月球着陆器处于某种 **状态s**时，我们就可以使用神经网络计算 $Q(s,a)$。

对于所有的4个动作，可以计算：$Q(s,nothing), Q(s,left),Q(s,main),Q(s,right)$，

最后，选择$Q(s,a)$值最大的那个动作。例如，假设对于状态$s_1$，这4个值中最大的是$Q(s_1,main)$，则我们选择该动作，也即启动主推进器。





### 2.2 如何训练一个神经网络来输出$Q(s,a)$

#### 2.2.1 描述

经过上述分析，问题就变成了我们如何训练一个神经网络来输出$Q(s,a)$。

事实证明，该方法将使用**贝尔曼方程**来创建包含大量样本 $(x,y)$ 的训练集，然后我们将使用监督学习，学习从$x$ 到 $y$ 的映射，即从 **（状态，动作）** 到目标值 **$Q(s,a)$** 的映射。



#### 2.2.2 如何得到一个训练集

**贝尔曼方程** 如下：
$$
Q(s,a) = R(s) + \gamma max_{a^{'}} Q(s^{'},a^{'})
$$
**右边** 是我们想要计算得到的 $Q(s,a)$ 的值，所以我们将右边的这个值称为目标值 $y$，而 $(s,a)$ 作为输入。 

神经网络的工作是输入x，即输入**状态动作对**，并尝试准确预测右边的值，即$Q(s,a)$。



##### 通过随机尝试得到各类样本

使用月球着陆器并尝试在其中执行不同的动作，如果我们还没有一个好的策略，我们将随机采取动作。通过在月球着陆器模拟器中尝试不同的事情，我们将观察到很多示例，说明我们合适处于某种状态并采取了一些行动，可能是一个好的行动，也可能是一个糟糕的行动。然后，由于处于该状态，我们获得了该状态的一些奖励，作为每个行动的结果，同时，通过采取行动，我们会进入某个新的状态$s^{'}$。即在每个状态采取不同的动作时，我们会看到一个以下元组：
$$
(s,a,R(s),s^{'})
$$


##### 如何计算贝尔曼方程中的 $Q(s^{'},a^{'})$——随机猜测



#### 2.2.3 学习算法——`DQN`

![image-20240123195513991](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123195513991.png)

- 首先，我们将采用上述神经网络并**随机初始化**神经网络的所有参数，作为对$Q(s,a)$ 的随机猜测
- 重复执行以下操作：
  - 使月球着陆器采取随机行动，并得到对应的元组 $(s,a,R(s),s^{'})$
  - 存储最近的 10000个元组
  - 训练神经网络：
    - 使用10000个元组创建包含10000个样本的训练集，其中：
      - 输入特征：$x=(s,a)$
      - 目标值：$y=R(s)+\gamma max_{a^{'}}Q(s^{'},a^{'})$
      - 其中，计算目标值所需要的 $Q(s^{'},a^{'})$
        - 在第一次时，将使用我们随机初始化之后的神经网络来计算得到，即进行猜测
        - 之后，使用训练过后的神经网络来计算得到
    - 使用创建好的训练集来训练神经网络，得到新的神经网络 $Q_{new}$
    - 更新神经网络 $Q=Q_{new}$





事实证明，如果我们从真正的随机猜测$Q$ 开始运行此算法，然后使用 贝尔曼方程反复尝试改进 $Q$函数的估计值，通过反复执行此操作，采取大量操作，训练模型，这将改进我们对 $Q$函数的猜测。

对于我们训练的下一个模型，由于现状已经对 $Q$函数有了更好的估计，则训练的下一个模型将会更好。并且通过更新神经网络 $Q=Q_{new}$，然后在下一次训练模型中，在创建训练集中计算目标值时，$Q(s^{'},a^{'})$ 将会是一个更好的估计。

当我们在每次迭代中运行这个算法，$Q(s^{'},a^{'})$ 将会成为对$Q$函数的更好估计，那么当运行算法足够长的时间时，这实际上将成为对 $Q$函数真实值的一个很好的估计，这样我们就可以使用它来计算 $Q(s,a)$，并挑选好的动作。



### 2.3 对神经网络架构进行改进

在上述算法中，我们使用了以下的神经网络架构（它将输入（状态，动作）对并尝试输出 $Q(s,a)$

![image-20240123191632989](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123191632989.png)

 使用这个神经网络，每当我们处于某个状态$s$时，我们都必须在神经网络中分别进行4次推理来计算这4个值，以便选择给我们提供最大$Q(s,a)$ 值的动作 $a$。这是低效的，因为我们必须才能每个状态进行4次推理。

相反，事实证明，训练单个神经网络同时输出所有这4个值会更有效，即使用下列神经网络：

![image-20240123231507186](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240123231507186.png)

其中输入是对应于月球着陆器**状态**的向量（包含8个数字），然后它通过神经网络：

- 第一个隐藏层有64个神经单元
- 第二个隐藏层有64个神经单元
- 输出层有4个神经单元，分别输出 $Q(s,nothing)、Q(s,left)、Q(s,main)、Q(s,right)$

该神经网络的工作是同时计算我们处于状态$s$时所有4种可能动作的$Q(s,a)$值。

事实证明，该神经网络架构更加有效，因为给定状态$s$，我们可以只运行一次推理便获得所有这4个值，然后非常快速地选择最大化$Q(s,a)$的动作$a$。



### 2.4 算法改进——$\epsilon$-贪婪策略

在上述的学习算法中，即使我们还在学习如何近似 $Q(s,a)$，也需要在月球着陆器中采取一些行动。当我们还在学习的时候，我们如何选择这些动作？

- 最常见的方法是使用 `Epsilon-greedy`策略



![image-20240124102651952](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240124102651952.png)

如图中所示，在我们的学习算法中，在学习的过程中需要使月球着陆器采取一些行动。



#### 采取行动的不同策略

1. 选择一个能够提供最大 $Q(s,a)$值的动作$a$，即使此时$Q(s,a)$不是$Q$函数的一个很好的估计，我们也使用当前对$Q(s,a)$的猜测并选择最大化它的动作$a$；
2. $0.95$概率的选择最大化$Q(s,a)$的动作$a$，$0.05$概率的随机选择一个动作$a$
   - 因为如果我们在随机初始化时，可能会使学习算法认为动作$a_j$绝对不是最好的行动，即在初始化神经网络时，使得$Q(s,a_j)$ 的值总是很低。在这种情况下，如果我们使神经网络总是选择最大化$Q(s,a)$的动作，则它永远不会尝试动作$a_j$。但是因为神经网络永远不会尝试该动作$a_j$，则它永远不会知道在某些状态$s_i$下，该动作$a_j$实际上是最好的行动。
   - 所以为了避免这种情况，我们使得有 $0.05$的概率去尝试不同的操作，使神经网络克服初始化时带来的先入之见。
   - 这种随机选择动作的想法被称为 **探索步骤**，因为我们要尝试一些可能不是最好的动作，但我们只是在某些情况下尝试一些行动。
   - 采取最大化$Q(s,a)$的行动，这被称为**贪婪行动**，因为我们试图通过选择它来实际最大化我们的回报。



在强化学习中，使用的技巧之一是从一个较高的 $\epsilon$ 值开始，最初，我们以一个较大的概率采取随机行动，然后逐渐减小这个概率，因此随着时间的推移，神经网络不太可能随机采取行动，而是更有可能使用当前对$Q(s,a)$的估计来选择好的行动。



### 2.5 算法改进——小批量和软更新

#### 2.5.1小批量

当数据集非常大时，梯度下降的每一步都需要计算非常多样本的损失的平均值，这是一个非常消耗资源和时间的步骤。

**小批量** 梯度下降的想法是在**每次迭代**中并不是使用所有的训练样本。相反，而是选择一个较小的训练样本子集，则每次迭代时梯度下降计算只需要计算该子集中的所有样本的损失的平均值以及导数值，这花费的时间较小，从而使得算法更加高效。



平均来说，**小批量**梯度下降会趋向于全局最小值，但具有不可靠性以及较多的嘈杂。但其每次迭代的计算成本要低很多，因此当有一个非常大的训练集时，小批量学习或小批量梯度下降被证明是一种更快的算法。



#### 2.5.2 软更新

在更新网络参数时，由于小批量梯度下降的嘈杂性或其他原因，可能经过一次训练得到的新的神经网络的参数可能更差，则直接使用：
$$
\matrix{W} = \matrix{W}_{new}
$$

$$
\matrix{B} = \matrix{B}_{new}
$$

将可能使神经网络变得更差。

对于**软更新**，则并不是直接进行如上的赋值更新，而是如下：
$$
\matrix{W} = 0.01\matrix{W}_{new} + 0.99\matrix{W}
$$

$$
\matrix{B} = 0.01\matrix{B}_{new}+0.99\matrix{B}
$$

