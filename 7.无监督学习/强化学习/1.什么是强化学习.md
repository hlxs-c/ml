# 什么是强化学习

## 1.一些概念

**强化学习**（`Reinforcement Learning`）是机器学习的一个分支，它关注如何使智能体（`agent`）通过与环境的交互来学习最优的行为策略。**在强化学习中，智能体通过观察环境的`状态`，采取特定的`动作`，然后接收来自环境的`奖励或惩罚`，进而调整其行为以最大化长期累积的`奖励`**。

**强化学习**的核心思想是通过试错探索和反馈机制来学习。智能体在与环境的交互过程中，通过尝试不同的动作并观察结果，逐渐学习到哪些动作可以获得更大的奖励或减少惩罚。通过不断优化策略和行为，智能体逐渐提高其在特定任务中的表现。

强化学习的关键概念包括：

1. **环境**（Environment）：智能体所处的外部环境，其状态可能随着时间的推移而改变。
2. **状态**（State）：环境的描述，它包含了智能体在某一时刻观察到的信息。
3. **动作**（Action）：智能体根据其策略选择的行为，其目标是通过选择最佳动作来最大化预期奖励。
4. **奖励**（Reward）：环境根据智能体的动作和状态反馈的信号，用于衡量动作的好坏。
5. **策略**（Policy）：智能体选择动作的决策规则，可以是确定性的或概率性的。
6. **值函数**（Value Function）：衡量在特定状态或状态-动作对下的长期奖励期望值，用于评估动作或策略的好坏。
7. **强化学习算法**：用于根据智能体与环境交互数据进行学习和优化的算法，例如`Q-learning`，`Deep Q-Network（DQN）`、`Policy Gradient`等。



**强化学习**的一个关键输入是一种叫做**奖励或奖励函数**的东西，它会告诉智能体什么时候做得好，什么时候做得不好。

**强化学习**不需要我们告诉算法每个输入的正确输出$y$是什么，我们要做的就是指定一个奖励函数，告诉它什么时候做得好，什么时候做得不好。





## 2.一个简单例子

### 2.1 强化学习应用程序工作的形式

假设有6个位置，火星车初始位置在第4个位置，火星车的位置在强化学习中称为**状态**，则共有6个可能状态：`state1,state2,state3,state4,state5,state6`：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116165847492.png" alt="image-20240116165847492" style="zoom:50%;" />

其中，`state1`（位置1）中更有研究价值，故其奖励为`100`，`state6`（位置6）也有一些研究价值，奖励为`40`，而其余位置没有研究价值，则奖励均为`0`：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116170101498.png" alt="image-20240116170101498" style="zoom:50%;" />

且`state1`和`state6` 称为`终端状态`，即当火星车到达这两个状态其中之一后，火星车就不会再进行移动，即不会再获得任何奖励。

在每一步中，火星车都可以选择两个动作之一：

- 向左走
- 向右走

假设火星车有以下行动策略以及对应获得的奖励：

| 行动策略           | 过程中获得的奖励序列       |
| ------------------ | -------------------------- |
| 左、左、左         | 0（初始）、0、0、100       |
| 右、右             | 0（初始）、0、40           |
| 右、右、左、左、左 | 0（初始）、0、0、0、0、100 |

在每个时间，火星车都处于某种状态，称为$S$，它可以选择一个动作$A$，并且它还可以获得一些奖励 $R(S)$，以及到达一个新的状态 $S^`$。例如：（4, 左，0，3），当我们了解特定的强化学习算法时，会在每次行动时都会注意到这4个要素（状态、动作、奖励、下一个状态），并且这4个要素是决定如何行动的关键要素。



### 2.2 强化学习的回报

在上述例子中，进行不同的行动策略，在过程中获得的奖励序列是不同的，要**衡量不同的奖励序列的好坏** 需要定义一个指标：**回报**。

> **回报**的概念强调可以更快获得的奖励可能比需要更长时间获得的奖励更有吸引力。

**回报** 被定义为**奖励序列**的加权和，其中权重由一个称为**折扣因子**的附加因素给出。

- **折扣因子**是一个略小于1的数字，记为 $r$

- $$
  回报 = R_1 + r*R_2 + r^2 * R_3 + ...
  $$

  

将上述的行动策略-奖励序列表格中添加一列**回报**的列以衡量不同的奖励序列（行动策略）的好坏：

> 注：下列使用 $折扣因子=0.9$
>
> 在许多强化学习算法中，折扣因子的一个常见选择是非常接近1的数字，例如0.9，0.99，甚至是0.999

| 行动策略           | 过程中获得的奖励序列       | 回报                                                         |
| ------------------ | -------------------------- | ------------------------------------------------------------ |
| 左、左、左         | 0（初始）、0、0、100       | 0+$(0.9)$ *0+$(0.9)^2$ *0 + $(0.9)^3$ * 100 = 72.9           |
| 右、右             | 0（初始）、0、40           | 0+$(0.9)$ *0+$(0.9)^2$ *40 = 32.4                            |
| 右、右、左、左、左 | 0（初始）、0、0、0、0、100 | 0+$(0.9)$ *0+$(0.9)^2$ *0 + $(0.9)^3$ *0 + $(0.9)^4$ * 0 + $(0.9)^5$ * 100 = 59.049 |



**得到的`回报` 取决于奖励，而`奖励`取决于采取的行动，因此`回报`取决于`采取的行动`**。

总而言之，强化学习中的回报是系统获得的奖励总和，由折扣因子加权，其中遥远的未来的奖励由提高到更高次幂的折扣因子加权。

但如果由任何奖励是负的，那么折扣因子实际上会激励系统将负奖励尽可能推迟到未来。 



### 2.3 决策：强化学习算法中的策略

#### 2.3.1 策略的简单描述

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116173645924.png" alt="image-20240116173645924" style="zoom: 80%;" />

如上图所示，火星车可以采取不同的策略以控制其在不同状态下的行动决策，如上图的策略有：

- 策略1：总是往最近的奖励走
- 策略2：总是往左走（总是往奖励最大的方向走）
- 策略3：总是往右走（总是往奖励最小的方向走）
- 策略4：总是往左走，除非其右边下一个状态就含有奖励



 **在强化学习中，我们的目标是提出一个称为 策略$Pi$ 的函数，其工作是将任何状态$s$ 作为输入并将其映射到它希望我们采取的某个动作$a$**：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116173938878.png" alt="image-20240116173938878" style="zoom:80%;" />

例如，对于上图中的策略4，这个策略会告诉火星车说如果处于状态2，那么它将决策向左走，即$\pi(2)=左$，同理有：$\pi(3)=左、\pi(4)=左、\pi(5)=右$。

策略函数$\pi$ 应用于状态$s$，告诉我们它在该状态下采取什么行动。

![image-20240116174228018](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116174228018.png)



#### 2.3.2 强化学习的目标

**强化学习的目标是找到一个策略 $\pi(s)$ 以告诉智能体在每个状态下采取什么行动以最大化回报**。

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116174441541.png" alt="image-20240116174441541" style="zoom:80%;" /> 





#### 2.3.3 回顾关键概念

![image-20240116175018651](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116175018651.png)



#### 2.3.4 马尔可夫决策

 以上分析的强化学习应用程序的形式实际上为 **马尔可夫决策过程**——**未来仅取决于当前状态，而不取决于在到达当前状态之前可能发生的任何事情**。

换句话说，在马尔可夫决策过程中，未来只取决于现在所处的位置（状态），而不取决于是如何到达该位置（状态）的。

![image-20240116175351633](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240116175351633.png)

