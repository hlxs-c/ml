# 异常检测算法实现实例

以下**异常检测**均使用 **密度估计** 的方法进行，且均使用**参数估计**的方法估计数据的**高斯分布**。即在数据集上，拟合**高斯分布**，然后找到概率非常低的值，并将其视为**异常**。



## 1. 异常检测用于2维特征数据集

在本示例中，我们将实现 **异常检测**算法来检测服务器计算机中的异常行为。



### 1.1 数据集

#### 1.1.1 数据集描述

数据集包含两个特征：

- 吞吐量（MB/s）
- 每个服务器响应的延迟（毫秒）

当服务器运行时，我们收集了 $m=307$ 个样本，因此有一个未标记的数据集 $\{x^{(1)},...,x^{(m)} \}$，我们怀疑这些样本中的绝大多数都是服务器正常运行的样本（非异常），但在此数据集中也可能存在一些服务器异常运行的样本。



#### 1.1.2 加载数据集

```python
# Load the dataset
X_train, X_val, y_val = load_data()
```

上述代码中的 `loda_data()` 将数据加载到变量中：

- `X_train` 作为训练集，用于拟合高斯分布
- `X_val` 和 `y_val` 作为交叉验证集来**选择阈值**并**确定异常样本与正常样本**



#### 1.1.3 查看数据集

查看数据集的一个方法是**打印出每个变量并查看它所包含的内容**：

```python
#Display the first five elements of X_train
print("The first 5 elements of X_train are:\n", X_train[:5])

#Display the first five elements of X_val
print("The first 5 elements of X_val are:\n", X_val[:5])

#Display the first five elements of y_vala
print("The first 5 elements of y_val are:\n", y_val[:5])
```



查看数据集的另一个有效的方法是**查看数据集的维度**：

```python
print("The shape of X_train is :", X_train.shape)
print("The shape of X_val is :", X_val.shape)
print("The shape of y_val is :", y_val.shape)
```



在开始执行任何任务之前，通过**可视化数据来了解数据通常很有用**：

对于**只有两个特征的数据集**，我们可以使用**散点图**来可视化数据（`X_train`），

```python
# Create a scatter plot of the data. To change the markers to blue "x",
# we used the 'marker' and 'c' parameters
plt.scatter(X_train[:, 0], X_train[:, 1], marker='x', c='b') 

# Set the title
plt.title("The first dataset")
# Set the y-axis label
plt.ylabel('Throughput (mb/s)')
# Set the x-axis label
plt.xlabel('Latency (ms)')
# Set axis range
plt.axis([0, 30, 0, 30])
plt.show()
```

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240127113607199.png" alt="image-20240127113607199" style="zoom:67%;" />



### 1.2 高斯分布

如果要执行异常检测，首先需要将模型拟合到数据的分布：

- 给定一个训练集 $\{x^{(1)},...,x^{(m)} \}$，我们要估计每个特征 $x_j$ 的 **高斯分布**

- 高斯分布由下式给出：

  - $$
    p(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}exp^{-\frac{(x-\mu)^2}{2\sigma^2}}
    $$

  - 其中 $\mu$ 是均值，$\sigma^2$ 是方差

- 对于每个特征 $i=1,...,n$，我们需要找到参数 $\mu, \sigma$ 拟合各个特征维度的数据



#### 1.2.1 估计高斯参数——参数估计

我们需要完成 `estimate_gaussian` 的代码，以计算 `mu`（每个特征的平均值）和`var`（每个特征的方差）：

- 计算均值的公式：

  - $$
    \mu_i = \frac{1}{m}\sum_{j=1}^m x_i^{(j)}
    $$

- 计算方差的公式：

  - $$
    \sigma_i^2 = \frac{1}{m} \sum_{j=1}^m (x_i^{j} - \mu_i)^2
    $$



```python
def estimate_gaussian(X):
    """
    Calculates mean and variance of all features in the dataset
    
    Args:
    	X(ndarray(m,n)):		Data matrix
    Returns:
    	mu(ndarray(n)):			Mean of all features
    	var(ndarray(n)):		Variance of all features
    """
    m, n = X.shape
    
    mu = 1/m * np.sum(X, axis=0)
    var = 1/m * np.sum((X-mu)**2, axis=0)
    
    return mu, var
```



#### 1.2.2 拟合高斯分布

在完成了参数估计 `estimate_gaussian` 函数之后，就可以进行拟合高斯分布，得到概率密度函数：

```python
def multivariate_gaussian(X, mu, var):
    """
    Computes the probability density function of the examples X under the multivariate gaussian distribution with parameters mu and var.
    If var is matrix, it is treated as the convariance matrix.
    If var is a vector, it is treated as the var values of the variances in each dimension (a diagonal covariance matrix).
    """
   	k = len(mu)
    
    if var.ndim == 1:
        var = np.diag(var)
    
    X = X - mu
    p = (2*np.pi)**(-k/2) * np.linalg.det(var)**(-0.5) * np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))
    
    return p
```



可视化高斯分布：

```python
def visualize_fit(X, mu, var):
    """
    This visualization shows you the 
    probability density function of the Gaussian distribution. Each example
    has a location (x1, x2) that depends on its feature values.
    """
    
    X1, X2 = np.meshgrid(np.arange(0, 35.5, 0.5), np.arange(0, 35.5, 0.5))
    Z = multivariate_gaussian(np.stack([X1.ravel(), X2.ravel()], axis=1), mu, var)
    Z = Z.reshape(X1.shape)

    plt.plot(X[:, 0], X[:, 1], 'bx')

    if np.sum(np.isinf(Z)) == 0:
        plt.contour(X1, X2, Z, levels=10**(np.arange(-20., 1, 3)), linewidths=1)
        
    # Set the title
    plt.title("The Gaussian contours of the distribution fit to the dataset")
    # Set the y-axis label
    plt.ylabel('Throughput (mb/s)')
    # Set the x-axis label
    plt.xlabel('Latency (ms)')
```

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240128112328291.png" alt="image-20240128112328291" style="zoom:67%;" />



### 1.3 选择阈值 $\epsilon$

现在我们已经估计了高斯参数，通过高斯分布的概率密度分布函数$p$ ，我们可以计算出哪些样本具有非常高的概率，哪些样本具有非常低的概率：

- 其中低概率样本更有可能是我们数据集中的异常
- 确定哪些样本是异常的一种方法是 **基于交叉验证集选择阈值**



我们需要完成函数 `select_threshold`来根据交叉验证集上的`F1`分数来选择阈值 $\epsilon$：

- 为此，我们将使用交叉验证集 $\{x_{cv}^{(1)},y_{cv}^{(1)},...,x_{cv}^{(m)},y_{cv}^{(m)} \}$，其中标签 $y=1$ 对应于异常样本， $y=0$ 对应于正常样本

- **基于交叉验证集的`F1`分数选择阈值**：

  - 遍历阈值，对于每个阈值，通过 $p(x) < \epsilon$ 来判断一个样本是否是异常样本，然后通过一下方式计算 **精确率** 和 **召回率**：

    - $$
      prec = \frac{tp}{tp+fp} \\
      rec = \frac{tp}{tp+fn}
      $$



```python
def select_threshold(y_val, p_val):
    """
    Finds the best threshold to use for selecting outliers based on the result from a validation set(p_val) and the ground truth(y_val)
    
    Args:
    	y_val(ndarray):		Ground truth on validation set
    	p_val(ndarray):		Result on validation set
    Returns:
    	epsilon(float):		Threshold chosen
    	F1(float):			F1 score by choosing epsilon as threshold
    """
    
    best_epsilon = 0
    best_F1 = 0
    F1 = 0
    
    step_size = (max(p_val) - min(p_val)) / 1000
    
    for epsilon in np.arange(min(p_val), max(p_val), step_size):
        # 根据当前阈值计算预测，其中，p(x)<epsilon，则认为其为异常，即其预测为1
        predictions = (p_val < epsilon)
        
        tp = np.sum((predictions==1) & (y_val==1))
        fp = np.sum((predictions==1) & (y_val==0))
        fn = np.sum((predictions==0) & (y_val==1))
        
        prec = tp / (tp+fp)
        rec = tp / (tp+fn)
        
        F1 = 2*prec*rec / (prec+rec)
        
        if F1 > best_F1:
            best_F1 = F1
            best_epsilon = epsilon
  
	return best_epsilon, best_F1
```



### 1.4 总过程

```py
# Load the dataset
X_train, X_val, y_val = load_data()

# 参数估计
# Estimate mean and variance of each feature
mu, var = estimate_gaussian(X_train) 

# 基于交叉验证集选择阈值
# Returns the density of the multivariate normal
# at each data point (row) of X_val
p_val = multivariate_gaussian(X_val, mu, var)
epsilon, F1 = select_threshold(y_val, p_val)

# 预测结果
# Find the outliers in the validation set 
outliers = p_val < epsilon
```





## 2. 异常检测用于高维数据集

```py
# load the dataset
X_train_high, X_val_high, y_val_high = load_data_multi()

print ('The shape of X_train_high is:', X_train_high.shape)
print ('The shape of X_val_high is:', X_val_high.shape)
print ('The shape of y_val_high is: ', y_val_high.shape)

# Apply the same steps to the larger dataset

# Estimate the Gaussian parameters
mu_high, var_high = estimate_gaussian(X_train_high)

# Evaluate the probabilites for the training set
p_high = multivariate_gaussian(X_train_high, mu_high, var_high)

# Evaluate the probabilites for the cross validation set
p_val_high = multivariate_gaussian(X_val_high, mu_high, var_high)

# Find the best threshold
epsilon_high, F1_high = select_threshold(y_val_high, p_val_high)

print('Best epsilon found using cross-validation: %e'% epsilon_high)
print('Best F1 on Cross Validation Set:  %f'% F1_high)
print('# Anomalies found: %d'% sum(p_high < epsilon_high))
```

