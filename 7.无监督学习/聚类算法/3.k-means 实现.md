# k-means 实现

## 1. 算法描述

`k-means`算法是一种自动将相似数据点聚类在一起的方法。

- 具体来说，我们得到了一个训练集 $\{x^{(1)},...,x^{(m)} \}$，并且我们想要把数据分组到几个相似的“集群”中

- `k-means` 是一个**迭代过程**：

  - 首先猜测初始质心

  - 然后重复以下过程完善此猜测：

    - 将样本点分配给最近的质心
    - 根据分配的样本点重新计算质心并更新质心

  - `k-means`算法的伪代码如下：

    - ```python
      # 初始化质心
      # K 是集群的个数
      centroids = kMeans_init_centroids(X, K)
      
      for iter in range(iterations):
          # Cluster assignment step
          # Assign each data point to the closest centroid
          # idx[i] corresponds to the index of the centroid
          # assigned to example i
          idx = find_closest_centroids(X, centroids)
          
          # Move centroids step:
          # Compute menas based on centroid assignments
          centroids = compute_menas(X, idx, K)
      ```

  - 算法的内部循环重复执行两个步骤：

    - 将每个样本$x^{(i)}$ 分配到其最接近的质心
    - 使用分配给每个质心的数据点计算平均值，并更新质心为平均值

  - `k-means`算法将始终收敛到质心的最终均值集，但是，收敛解可能并不总是理想的，这取决于质心的初始化。因此，在实践中，`k-means`算法通常以不同的方式随机初始化运行几次，然后从不同的随机初始化得到的解决方案中选择一个具有最低成本函数值（失真）的方案。



## 2.算法实现

### 2.1 查找最接近的质心

在`k-means`算法的“聚类分配”阶段，该算法根据质心的当前位置，将每个训练样本 $x^{(i)}$ 分配给其最接近的质心。

我们要完成上述算法描述中伪代码中的 `find_closest_centroids`函数：

- 该函数获取数据矩阵 $\matrix{X}$ 和 内部所有质心 $centroids$ 的位置

- 它应该输出一个一维数组（其元素数量与$\matrix{X}$含有的样本数量相同），该数组`idx`包含每个训练样本最接近的质心的索引，即`idx[i] = 1` 代表与样本$x^{(i)}$ 最接近的质心是 $1$号质心，即最接近的质心是 `centroids[1]`

- 具体来说，对于我们的每个训练样本$x^{(i)}$：

  - $$
    c^{(i)} = j \space that  \space minimizes \space \space ||x^{(i)}-\mu_j||
    $$

  - $c^{(i)}$ 是最接近样本$x^{(i)}$的质心的索引，对应于代码中的 `idx[i]`

  - $\mu_j$ 是第$j$ 个质心的值，存储在 `centroids`



```python
# GRADED FUNCTION: find_closest_controids

def find_closest_centroids:
    """
    computes the centroid memberships for every example
   	Args:
   		X(ndarray(m,n)):	Input examples
   		centroids(ndarray(k)):	K centroids
   	Returns:
   		idx(array(m)):	cloest centroids index
    """
    
    # Set K
    K = centroids.shape[0]
    
    # idx[i]存储最接近样本X[i]的质心的索引
    idx = np.zeros(X.shape[0], dtype=int)
    
    # 遍历所有样本点
    for i in range(X.shape[0]):
        # 开辟一个临时数组distance存储样本X[i]距离每个质心的距离
        distance = []
        # 遍历所有质心
        for j in range(centroids.shape[0]):
            norm_ij = np.linalg.norm(X[i] - centroids[j])
            distance.append(norm_ij)
        # 选择最接近的质心
        idx[i] = np.argmin(distance)
```



### 2.2 计算分配给某个质心的样本点的平均值——更新质心

将每个样本点分配给其最接近的质心之后，算法的第二阶段是针对每个质心，计算分配给它的样本点的平均值，并更新其为平均值。

我们要完成上述算法描述中伪代码中的 `compute_centroids`函数，以重新计算每个质心的值：

- 具体来说，对于每个质心$\mu_k$

  - $$
    \mu_k = \frac{1}{|C_k|} \sum_{i ∈ C_k} x^{(i)}
    $$

  - 其中，$C_k$ 是分配给第$k$个质心的所有样本点

  - $|C_k|$是分配给第$k$个质心的样本点的数量

- 具体来说，如果有两个样本$x^{(3)}, x^{(5)}$ 分配给了第2个质心，则更新步骤为：

  - $$
    \mu_2 = \frac{1}{2} (x^{(3)} + x^{(5)})
    $$

    

```python
# GRADED FUNCTION: compute_centroids

def compute_centroids:
    """
    Returns the new centroids by computing the means of data points assigned
    to each centroid
    
    Args:
    	X(ndarray(m,n)):	Data points
    	idx(ndarray(m)):	Array containing index
    """
    
    # Useful variables
    m, n = X.shape
    
    # centroids数组存储新的质心
    centroids = np.zeros((K,n))
    
    # 遍历所有质心
    for k in range(K):
        # 获取所有分配给第k个质心的样本点
        points = X[idx==k]
        # 计算points的平均值并更新第k个质心
        centroids[k] = np.mean(points, axis=0)
   
	return centroids
```



### 2.3 实现`k-means`

```python
def run_kMeans(X, initial_centroids, max_iters=10, plot_progress=False):
    """
    Runs the K-Means algorithm on data matrix X, where each row of X is single example
    """
    
    # Initialize values
    m, n = X.shape
    K = initial_centroids.shape[0]
    centroids = initial_centroids
    previous_centroids = centroids
    idx = np.zeros(m)
    
    # Run K-Means
    for i in range(max_iters):
        # Output progress
        print("K-Means iteration %d/%d" %(i, max_iters-1))
        
        # For each example in X, assign it to the cloest centroid
        idx = find_closest_centroids(X, centroids)
        
        # Optionally plot progress
        if plot_progress:
            plot_progress_kMeans(X, centroids, previous_centroids, idx, K, i)
            previous_centrois = centroids
        
        # Given the memberships, compute new centroids
        centroids = compute_centroids(X, idx, K)
   	
    plt.show()
    return centroids, idx
```



### 2.4 调用`k-means`算法

```python
# Load an example dataset
X = load_data()

# Set initial centroids
initial_centroids = np.array([[3,3], [6,2], [8,5]])
K = 3

# Number of iterations
max_iters = 10

centroids, idx = run_kMeans(X, initial_centroids, max_iters, plot_progress=True)
```





### 2.5 随机初始化

在实践中，**初始化质心的一个好策略是从训练集中选择随机样本**。

我们要实现 `kMeans_init_centroids`函数：

- 首先需要随机打乱样本的索引
- 然后，它根据索引的随机排列选择前`k`个样本
  - 这允许随机选择样本，而不会出现有两次选择同一个样本的风险



```python
def kMeans_init_centroids(X, K):
    """
    This function initializes K centroids that are to be used in K-Means on the dataset X
    
    Args:
    	X(ndarray):		Data points
    	K(int):			number of centroids/clusters
    Returns:
    	centroids(ndarray):	Initialized centroids
    """
    
    # Randomly reorder the indices of examples
    randidx = np.random.permutation(X.shape[0])
    
    # Take the first K examples as centroids
    centroids = X[randidx[:K]]
    
    return centroids
```





## 3. 使用`k-means` 进行图像压缩

在图像的简单24位颜色表示中，每个像素表示为3个8位无符号整数（范围从0到255），它们指定红色、绿色和蓝色强度值。这种编码通常称为`RGB`编码。

我们的图像包含数千种颜色，在该实例中，我们将使用`k-means`算法将颜色的数量减少到16种颜色。通过进行这种减少，可以有效地压缩图片。

具体来说，将颜色减少到16种之后，我们只需要存储16种选定的颜色的`RGB`值，对于每个像素，我们只需要存储该位置对应颜色的索引（其中只需要用4位来表示16个索引）。



我们使用`K-Means`算法来选择将用于表示压缩图像的`16`种颜色：

- 具体来说，我们需要将原始图像的每个像素视为数据样本，并使用`K-Means`算法查找16种颜色，这些颜色可以最好地对3维`RGB`空间中的像素进行分组（聚类）。
- 计算出图像上的聚类质心之后，将使用16种颜色替换原始图像中的像素的颜色。



### 3.1 数据集

首先，我们将使用`matplotlib`读取原始图像：

```python
# Load an image of a bird
original_img = plt.imread('bird_small.png')
```

可视化图像：

```python
# Visualizing the image
plt.imshow(original_img)
```

**检查数据的维度**：

```python
print("Shape of original_img is:", original_img.shape)
```

输出为：

```
Shape of original_img is: (128, 128, 3)
```

如上所示，这创建了一个三维矩阵`original_img`，其中：

- 前两个索引标识像素的位置
- 第三个索引标识红色、绿色、蓝色
- 例如，`original_img[50,32,2]`给出第50行第33列处像素的蓝色强度



### 3.2 预处理数据

要调用`run_kMeans`算法，首先需要将矩阵转换为二维矩阵，即将三维矩阵`(128,128,3)` 转换为 `(128x128, 3)`的二维矩阵：

```python
# Divide by 255 so that all values are in the range 0-1
original_img = original_img / 255

# Reshape the image into an mx3 matrix where m = number of pixels
# (in this case m = 128x128 = 16384)
# Each row will contain the Red, Green and Blue pixel values
# This gives us our dataset matrix X_img that we will use K-Means on.

X_img = np.reshape(original_img, (original_img.shape[0] * original_img.shape[1]), 3)
```



### 3.3 运行`K-Means`算法

```python
# Run K-Means algorithm on this data
# You should try different values of K and max_iters here
K = 16
max_iters = 10

# 初始化质心
initial_centroids = kMeans_init_centroids(X_img, K)

# Run K-Means
centroids, idx = run_kMeans(X_img, initial_centroids, max_iters)
```



### 3.4 压缩图像

找到用于表示压缩图像的`16`种颜色之后，可以使用 `find_closest_centroids` 函数将每个像素分配给其最接近的质心，即为每个像素分配给压缩后的颜色。

```python
# Represent image in terms of indices
X_recovered = centroids[idx, :]

# Reshape recovered image into proper dimensions
X_recovered = np.reshape(X_recoverd, original_img.shape)
```



### 3.5 压缩图像的解释

上述使用`16`种颜色进行图像压缩，并存储颜色字典和每个像素对应颜色的索引是一种常见的方法：

1. **颜色字典**：在进行图像压缩之前，需要创建一个颜色字典，这个字典包含了16种颜色，每种颜色用`RGB`值表示。这些颜色通过聚类算法（`k-means`算法）生成。
2. **压缩过程**：在压缩图像时，对于每个像素，需要找到与其最接近的颜色，并记录其在颜色字典中的索引。最常用的方法是计算每个像素与16中颜色之间的欧式距离，并选择距离最小的颜色作为该像素的代替颜色。
3. **索引存储**：对于每个像素，只需要使用`4`位二进制来存储其在颜色字典中的索引。由于有16种颜色，所以用4位二进制足够表示0到15的索引。
4. **存储空间**：在压缩之后，需要存储压缩的图像数据和颜色字典。对于图像数据，每个像素只需要用4位二进制来表示索引，因此存储空间大大减小。而颜色字典需要额外的`16x24`位来存储16种颜色的`RGB`值。

需要注意的是，虽然这种方法可以实现图像压缩，但压缩比取决于选择的颜色数量。较少的颜色数量会导致更高的压缩比，但可能会引入更多的图像失真。因此，在选择颜色数量时需要权衡压缩比和图像质量。