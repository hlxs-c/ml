# 协同过滤算法

## 1. 举例说明

如果我们知道每一部电影的特征向量 $(x_1, x_2)$，则可以使用基本的线性回归来为不同的用户预测其对不同电影的评分。

但是，如果我们事先不知道这些特征的值，例如：

![image-20240115140558318](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240115140558318.png)

那应该怎么建立模型进行预测呢？



### 1.1 假设已经以某种方式学习了各个用户的参数

为了说明 **如何在不知道特征的值的情况下建立模型以预测不同用户对不同电影的评分** ，假设我们已经以某种方式为这4个用户学习了其各自的参数：
$$
\mathbf{w}^{(1)} = \begin{bmatrix}5 \\ 0 \end{bmatrix}, b^{(1)} = 0 \\
\mathbf{w}^{(2)} = \begin{bmatrix}5 \\ 0 \end{bmatrix}, b^{(2)} = 0 \\
\mathbf{w}^{(3)} = \begin{bmatrix}0 \\ 5 \end{bmatrix}, b^{(3)} = 0 \\
\mathbf{w}^{(4)} = \begin{bmatrix}0 \\ 5 \end{bmatrix}, b^{(4)} = 0
$$
因为我们有以上参数以及训练集中的标签值，则：
$$
\mathbf{w}^{(1)}·x^{(1)} + b^{(1)} = 5 \\
\mathbf{w}^{(2)}·x^{(1)} + b^{(2)} = 5 \\
\mathbf{w}^{(3)}·x^{(1)} + b^{(3)} = 0 \\
\mathbf{w}^{(4)}·x^{(1)} + b^{(4)} = 0 \\
$$
根据这些等式，可以估计出：
$$
x^{(1)} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$
类似地，通过利用各个用户的参数，也可以尝试为其他几部电影估计出对应的特征向量。

**注意**：这之所以有效，是因为我们有多个用户的参数以供我们估计每部电影的特征向量。但是在一个典型的线性回归应用程序中，如果只有一个用户，那么实际上是没有足够的信息来确定样本的特征向量的。而在协同过滤中，是因为有多个用户对同一部电影的同一项目的评分，从而可以尝试猜测这些特征的可能值。



为了让算法对这些电影的预测接近用户给出的实际评分，需要提出一个**损失函数** 来学习 $(x_1,x_2)$ 的值：

- **为了学习单独一部电影的特征向量$\mathbf{x}^{(i)}$ 的损失函数**：

$$
J(\mathbf{x}^{(i)}) = \frac{1}{2}\sum_{j:r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^n (x_k^{(i)})^2
$$

- **为了学习所有电影的特征向量 $\mathbf{x}^{(1)},\mathbf{x}^{(2)},...,\mathbf{x}^{(n_m)}$ 的损失函数**：

$$
J(\mathbf{x}^{(1)},\mathbf{x}^{(2)},...,\mathbf{x}^{(n_m)}) =  \\
\frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n (x_k^{(i)})^2
$$



所以，如果我们有所有用户的参数，然后使用梯度下降或其他优化算法来最小化上述的损失函数，则以猜测出各个电影的特征向量。



### 1.2 去除已经有了各个用户的参数的假设——使用协同过滤算法

上述我们猜测各个电影的特征向量的特征向量的方法建立在我们已经使用某种方法学习得到了各个用户对电影评分的参数 $(\mathbf{w},b)$，但是问题是我们该如何得到这些参数？

**方法**：将用于学习参数 $(\mathbf{w},b)$ 的算法与上述讨论的用于学习特征向量 $x^{(i)}$ 的算法结合在一起，即使用协同过滤算法。



下面分别是用于学习参数 $(\mathbf{w},b)$ 和用于学习特征向量 $x^{(i)}$ 的损失函数：

- 用于学习参数 $(\mathbf{w},b)$ 的损失函数：

  - $$
    min_{\mathbf{w}^{(1)},b^{(1)},...,\mathbf{w}^{(n_u)},b^{(n_u)}} 
    \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac {\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(w_k^{(j)})^2
    $$

    

- 用于学习特征向量 $x^{(i)} | i=1,2,...,n_m$ 的损失函数：

  - $$
    min_{\mathbf{x}^{(1)},...,\mathbf{x}^{(n_m)}} \frac{1}{2}\sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n(x_k^{(i)})^2
    $$

- 



**注意到**：

-  $(\mathbf{w}^{(j)},b^{(j)})$ 和 $x^{(i)}$ 都未知

- $$
  \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2
  $$

- $$
  \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2
  $$

上述的这两个求和项代表的含义是相同的，都**代表模型对所有已经有评分（标签）的样本的预测的误差的平方和**，所以可以将两个损失函数结合在一起，则可以得到学习 $\mathbf{w}, b, \mathbf{x}$ 的总体损失函数：
$$
J(\mathbf{w},b,\mathbf{x}) = \\
\frac{1}{2} \sum_{(i,j):r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac {\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(w_k^{(j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n(x_k^{(i)})^2
$$
其中：项 $\sum_{(i,j):r(i,j)=1} (\mathbf{w}^{(j)}·\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2$ 代表对模型对所有已经有评分（标签）的样本的预测的误差的平方和；



![image-20240115145800876](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240115145800876.png)



### 1.3 如何最小化总体成本函数 $J(\mathbf{w},b,\mathbf{x})$

我们可以使用 **梯度下降** 算法来最小化该总体成本函数 $J(\mathbf{w},b,\mathbf{x})$：

-  在线性回归中，我们使用梯度下降算法来优化成本函数 $J(\mathbf{w},b)$，从而学习得到参数 $(\mathbf{w},b)$：

  - $$
    \text{repeat \{ } \\
    w_i = w_i - \alpha \frac{\delta J(\mathbf{w},b)}{\delta w_i} \\
    b = b - \alpha \frac{\delta J(\mathbf{w},b)}{\delta b}
    $$

    

- 在协同过滤中，由于总体成本函数 $J(\mathbf{w},b,\mathbf{x})$ 是关于 $(\mathbf{w},b)$ 和 $\mathbf{x}$ 的函数，且我们要学习的不仅仅只有参数 $(\mathbf{w},b)$，也需要学习 $\mathbf{x}$，故需要更改为如下形式：

  - $$
    \text{repeat \{ } \\
    w_i^{(j)} = w_i^{(j)} - \alpha \frac{\delta J(\mathbf{w},b,\mathbf{x})}{\delta w_i^{(j)}} \\
    b^{(j)} = b^{(j)} - \alpha \frac{\delta J(\mathbf{w},b,\mathbf{x})}{\delta b^{(j)}} \\
    x_k^{(i)} = x_k^{(i)} - \alpha \frac{\delta J(\mathbf{w},b,\mathbf{x})}{\delta x_k^{(i)}}  \\
    $$

  - **注意**：由于 $\mathbf{x}$ 也是我们需要学习的，所以在使用梯度下降优化总体成本函数 $J(\mathbf{w},b,\mathbf{x})$ 时，不仅要更新模型参数 $(\mathbf{w},b)$，还要更新 $\mathbf{x}$



## 2. 课外补充

**协同过滤**（Collaborative Filtering）是一种常用的推荐系统算法，用于预测用户可能喜欢的物品或项目。**它基于观察到的用户行为数据，如用户的评分、购买历史记录或浏览记录，来判断用户的偏好和相似性**。

**协同过滤算法** 的核心思想是基于用户行为的相似性或项目的相似性进行推荐。它假设用户具有相似的品味，如果两个用户在过去的行为中有相似的模式，那么他们很可能对未来的物品有相似的偏好。同样，如果两个物品经常被同一组用户喜欢或购买，那么它们可能在特征或内容上相似。因此对一个用户喜欢的物品进行推荐时，可以考虑该物品相似的其他物品。

协同过滤算法一般有两种主要的方法：

1. **基于用户的协同过滤**（User-Based Collaborative Filtering）：该方法首先计算用户之间的相似性，然后根据相似用户的行为和偏好，为目标用户推荐物品。这种方法的关键是找到与目标用户行为模式相似的其他用户，并基于相似用户的评分或行为历史来预测目标用户对尚未评分的物品的喜好程度。
2. **基于物品的协同过滤**（Item-Based Collaborative Filtering）：该方法首先计算物品之间的相似性，然后根据目标用户已经评分或喜欢的物品，找到相似的物品进行推荐。这种方法的关键是找到与目标物品相似的其他物品，并基于用户对相似物品的评分或行为历史来预测目标用户对尚未评分的物品的喜好程度