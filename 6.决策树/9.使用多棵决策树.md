# 使用多棵决策树

使用 **单棵**决策树的缺点之一是该决策树可能对数据中的微小变化高度敏感。

为了使模型不那么敏感或者更加健壮的一种解决方法是——不仅仅构建一棵决策树，而是构建很多决策树，称为**树集合**。



## 1.单棵决策树的缺点

![image-20240112153626294](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112153626294.png)

如上图所示，在猫分类器中，在原有数据集上，我们选用 **耳朵的形状** 作为根节点的拆分特征（即在原有数据集中，”耳朵的形状“ 作为根节点的拆分特征可以提供最大的信息增益），而当我们更改数据集中的一个样本，在新的数据集上，为根节点的拆分提供最大的信息增益的特征变为了 **是否有胡须**。即我们仅仅改变了一个训练样本，就导致了算法在根部产生不同的分裂，从而产生一颗完全不同的树，这使得该算法（模型）不那么健壮。



**这也是为什么当我们使用多棵决策树时，通常会得到更好的结果，也就是说，如果不仅仅训练单个决策树而是训练一大堆不同的决策树，则我们可能会得到更准确的预测。**



## 2.如何使用多棵决策树进行预测

如果我们拥有多棵决策树的集合，其中没棵决策树都是对猫与非猫进行分类的一种合理方法，如下托所示：

![image-20240112154313587](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112154313587.png)

如果有一个想要分类的新测试样本，那么我们要做的就是将这个新的测试样本输入到所有的三棵决策树，并让它们**投票** 决定最终的预测。

![image-20240112154514460](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112154514460.png)

如上图所示，在将右侧的新的测试样本输入到这三棵决策树中，三棵决策树的预测分别是 $1, 0, 1$，则多数票是 $y=1$，则最终的预测结果为 $y=1$。



使用 **树集合** 的原因是通过拥有大量决策树并让它们进行投票，使得整体算法对任何一颗树可能正在做的事情不太敏感，因为它只能获得 $\frac{1}{n}$ 的票，通过多棵决策树的投票，这使得我们的整体算法更加健壮。



