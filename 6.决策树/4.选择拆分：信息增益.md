# 选择拆分：信息增益

在构建决策树时，我们**在节点上选择什么特征进行拆分取决于选择什么特征可以最大程度地减少熵**（减少熵或减少杂质，或最大化纯度）。

在决策树的学习中，熵的减少称为**信息增益**。



> 本文描述如何计算**信息增益**，从而选择在决策树的每个节点上使用哪些特征进行拆分。



## 1.举例说明

下面以 **决定在猫分类器的决策树的根节点使用什么特征来识别猫与非猫** 为例：

![image-20240111165758192](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240111165758192.png)

如上图所示，我们遍历三个特征，并分别使用三个特征进行拆分，得到了不同的拆分结果，并计算了在不同拆分结果中**左右分支中的$p_1$ 值以及 对应的 熵值**，其中：

- $$
  p_1=\frac{正样本的数量}{样本的总数} \\
  在该例字中：p_1=\frac{猫的样本数量}{样本的总数}
  $$

- $$
  熵值 = H(p_1)
  $$

在计算出分别使用这三个特征进行拆分的结果以及对应**左右分支中的$p_1$值以及熵值**之后，需要回答的关键问题是，鉴于要在根节点使用的特征来说，这三个特征中哪一个最有效。

事实证明：与其查看这些熵值并进行比较，还不如对它们进行**加权平均**。

- 因为如果有一个节点中有很多样本且具有高熵值，这比如果有一个节点中有很少的样本且具有高熵值更加糟糕；
  - 因为熵作为杂质的度量，如果有一个非常大且不纯的数据集，那么与只有几个样本且和非常不纯的数据集相比，前者将更加糟糕



在使用某个特征进行拆分之后，得到的结果中将会有两个数字与之关联：左右分支的熵。为了从中挑选一个合适的特征进行拆分，我们应该将这两个与拆分结果相关联的数字组合为一个数字以进行评估拆分的好坏，而将这两个数字组合为一个数字的方法就是 **取加权平均值**。因为在左或右分支中具有低熵有多重要还取决于有多少样本进入了左或右分支。



**加权平均**的计算：

例如在使用耳朵形状这个特征进行拆分的第一个例子中，根节点中共有10个样本，使用该特征进行拆分之后，有5个样本进入了左分支，另外5个样本进入了右分支，则**取熵的加权平均值**为：
$$
熵的加权平均值= \frac{5}{10}H(0.8) + \frac{5}{10}H(0.2)
$$
依次类推，则可以得到如下图中的结果：

![image-20240111172009752](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240111172009752.png)

**根据我们的目标（最大化拆分之后的节点的纯度，最小化拆分之后的节点的杂质），我们选择拆分的方法是计算出这三种拆分方法中左右分支熵值的加权平均值，并选择其中最小的一个。**



但是在构建决策树的过程中，我们实际上将对这些公式进行更改，以遵循决策树构建中的惯例，但实际上并不会改变结果——与计算拆分结果的左右分支的熵值的加权平均值相比更好的方法是，**计算与没有分裂之前的熵值相比的减少量**。

例如，对于第一个例子中，根节点含有10个样本，且分别有5只猫和5只狗，即未分裂的根节点的$p_1 = 0.5$，也即未分裂的根节点的熵值为1。

要计算拆分后的结果与没有分裂之前的熵值的减少量，则公式如下：
$$
熵值减少量 = 未分裂之前的熵值 - 拆分结果的熵值 \\
= 未分裂之前的熵值 - 拆分结果中左右分支熵值的加权平均值 
$$
在该例子中为：
$$
熵值减少量=H(0.5)-(\frac{5}{10}H(0.8) + \frac{5}{10}H(0.2))=0.28
$$
依次类推，可以得到如下图所示的结果：

![image-20240111173020354](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240111173020354.png)

在上述图中计算的结果 $0.28、0.03、0.12$ 这些值，就被称为**信息增益**，**它衡量的是由于分裂而导致的树中熵值的减少**。



为什么要计算**熵的减少量** 而不是仅仅计算 **左右分支的熵值的加权平均值**？

- 事实证明，决定何时不再进一步分裂的停止标准之一是**熵的减少**是否太小
- 如果某一次分裂带来的熵的减少很小，那么这只是在不必要地增加树的大小，并冒着通过分裂过多而过度拟合的风险

在上述的例子中，由于使用 耳朵的形状 这个特征进行拆分得到的信息增益的值最大（熵的减少量最大），则应该选择 “耳朵的形状” 这个特征 对根节点进行拆分。



## 2.正式定义

定义：
$$
p_1^{root} = 根节点中正例的比例 \\
=\frac{根节点中正样本的数量} {根节点中样本的总数量}
$$

$$
p_1^{left} = 左子树中正例的比例 \\ 
=\frac {左子树中正样本的数量} {左子树中样本的总数量}
$$

$$
w^{left} = 根节点中样本划分到左子树中的比例 \\
=\frac{左子树中样本的总数量} {根节点中样本的总数量}
$$

同样的有：


$$
p_1^{right} = 右子树中正例的比例 \\ 
=\frac {右子树中正样本的数量} {右子树中样本的总数量}
$$

$$
w^{right} = 根节点中样本划分到右子树中的比例 \\
=\frac{右子树中样本的总数量} {根节点中样本的总数量}
$$

则**信息增益的计算公式**为：
$$
Information \space gain（信息增益） = H(p_1^{root})-(w^{left} H(p_1^{left}) + w^{right}H(p_1^{right}))
$$


**信息增益是在决策树算法中用于选择最佳划分特征的度量指标。它衡量了在划分前后数据集的信息不确定性减少的程度。**

在决策树中，我们希望找到最优的划分特征，使得在该特征的取值下，数据集能够被划分为尽可能纯净的子集。信息增益可以帮助我们评估每个特征的划分能力，选择能够最大程度减少数据集不确定性的特征。

信息增益的计算基于熵的概念。熵是用来度量数据集的不确定性的指标。熵值越高，表示数据集的混乱程度越大，不确定性也就越高。而决策树的目标是通过选择最佳划分特征来最大化信息增益，从而在每个节点上实现更好的分类。

信息增益的计算公式如下:
$$
信息增益 = 划分前的熵 - 加权划分后的熵（加权平均值）
$$
具体而言：

1. 首先计算划分前整个数据集的熵
2. 然后对于每个可能的划分特征，计算在该特征下划分后的子集的熵，并根据子集的大小进行加权平均
3. 最后，将划分前的熵减去加权平均后的熵，即得到信息增益的值

信息增益越大，表示划分后数据集的不确定性减少得越多，划分能力越好。

通过比较不同特征的信息增益，我们可以选择具有最大信息增益的特征作为划分依据，将数据集划分为更纯净的子集，从而构建出更好的决策树模型。

