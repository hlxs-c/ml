# 替换采样（有放回采样）与`随机森林`

## 1. 什么是替换采样

**替换采样**（Replacement Sampling），也称为**有回放采样**（Sampling with Replacement），是一种从数据集中进行采样的方法，其中每个样本在每次采样时都有可能被选中多次。

在替换采样中，假设我们有一个包含 $N$ 个样本的数据集。在每次采样时，我们从数据集中随机选择一个样本，并记录下来。然后，**将选中的样本放回数据集中**，使得该样本在下一次采样时仍然有可能被选中。这种**放回操作** 使得样本在每次采样时都是独立且具有相同的选中概率。

**替换采样的主要特点是样本的选中是独立的，即每次采样之间没有关联。这意味着同一个样本可以在多次采样中被选中，也可以在某次采样中被忽略。因此，样本在替换采样中可能被选中多次，也可能不被选中。**

替换采样在许多机器学习算法中被广泛使用，特别是基于集成学习的方法中，如随机森林。**通过替换采样，可以生成多个有差异的训练子集，每个子集都是从原始数据集中有放回地采样得到的。这样，可以在每个子集上独立地训练一个基学习器，并将它们组合起来形成一个强大的集成模型。**

需要**注意**的是，替换采样会引入样本之间的关联性较低，因为同一个样本可能会被选中多次，而其他样本可能在某次采样中被忽略。这可能对一些统计分析和假设产生影响，因此在使用替换采样时需要考虑这一点。



## 2.如何将 `替换采样` 应用于构建 `树的集合`

### 2.1 构建 `随机训练集`

我们将**构建多个随机训练集**，这些训练集都与原始训练集略有不同：

假设原始训练集的大小为 $N$，则我们可以在训练集中有放回地进行采样 $N$ 次，从而可以得到一个与原始训练集大小相同的 **随机训练集**，该 ”随机训练集“ 与原始训练集有些相似但也有不同。

通过多次 **对原始训练集有放回地采样$N$次**，就可以构建处多个 **随机训练集**。



### 2.2 构建 `树集成算法`——随机森林

**随机森林算法** 是一种强大的树集成算法，比使用单个决策树效果更好。



#### 2.2.1 袋装决策树

- 给定一个大小为 $M$ 的训练集；
- 使用 **替换采样** 从原始训练集中构建出 $B$ 个与原始训练集大小相同的 新训练集——**随机训练集**；
- 在每一个新的**随机训练集** 上训练一个决策树，得到 $B$ 个 决策树；
  - $B$ 的大小选择可能在 $100$ 左右；
  - 一般来说，建议使用 `[64, 228]` 中的值；
  - 事实证明，将$B$设置得很大不会影响整体算法的性能，但是当超过某个值后，最终会得到收益递减的结果，而且当 $B \gt \gt 100$ 时，模型的性能并不会变得更好，所以一般不建议将 $B$ 设置的很大，否则这只会显著降低计算速度，而不会显著提高整体算法的性能；
- 当构建出了具有 $B$ 棵决策树的树集合之后，使用这些决策树进行预测并最终通过投票得到最终预测；



#### 2.2.2 随机森林

通过对 **袋装决策树** 进行一些修改，可以将器修改为 **随机森林算法**。

**关键思想**是：即使使用 **替换采样**，有时最终还是会在根节点处始终使用相同的特征进行拆分，而且在跟节点附近使用非常相似的特征进行拆分。

因此，对 **袋装决策树**算法进行一项修改，已进一步尝试随机化每个节点的拆分特征的选择，使得树集合中的决策树彼此之间变得更加不同，从而使得最终进行投票预测时获得更加准确的预测。

通常这样做的**方法是**：在每个节点上选择某个特征进行拆分时，如果有 $n$ 个特征可供选择，则随机选出一个大小为 $k$ 的子集（其中 $k <n$），然后让算法只允许在这个子集中选择某个特征进行拆分。

当 可供选择的特征很多时（即$n$ 比较大，例如 $n$ 是几十甚至是数百），$k$ 值的典型选择是选择 $k = \sqrt{n}$。

![image-20240112164030476](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112164030476.png)

通过上述方法对 **袋装决策树** 进行修改，就可以得到 **随机森林算法**。



#### 2.2.3 健壮性更强的原因

 **随机森林** 比 单个决策树 更健状的原因是：

- 替换过程导致算法探索数据的许多微小变化

- 并且它训练不同的决策树并对所有这些树进行平均更改程序导致的数据变化

因此，这意味着训练集的任何微小变化都不太可能对整个 **随机森林算法** 的整体输出产生巨大影响。
