# 整合

## 1. 构建决策树的全过程

> **信息增益**标准让我们决定如何选择一个特征来分割一个节点，本文描述如何在决策树的多个地方使用它，以便弄清楚如何构建具有多个节点的大型决策树。

![image-20240112110001163](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112110001163.png)

**构建决策树的全过程（递归的一个过程）**：

1. 从根节点处的所有训练样本开始，**计算所有可能特征的信息增益**，并选择要拆分的特征，从而提供最高的信息增益；
2. 选择特征之后，根据所选的特征将根节点的数据集拆分为两个子集，并创建树的左右分支，并将训练样本发送到左分支或右分支（每个样本被划分到哪一个分支取决于其对应特征的值）；
3. 继续在树的左分支和右分支上重复拆分过程，直到满足选择的拆分的停止条件（一个或多个）：
   1. 当一个节点所含的所有训练样本属于同一个类别时（即一个节点只含有一个类别）；
   2. 当拆分一个节点将导致超过所设置的树的最大深度时；
   3. 当拆分一个节点获得的信息增益小于一个阈值时；
   4. 当一个节点中的样本数量小于一个阈值时；

以上就是构建决策树的全过程，可以看到，当我们决定在根节点上使用某个特征拆分之后，构建左子树或右子树的方式就是在对应的样本子集上构建决策树，也即这是一个递归的过程。这意味着构建决策树的方式是通过构建较小的决策树然后将它们放在一起来构建整体决策树。



## 2. 选择最大深度

1.决策树的**最大深度** 有许多不同的可能选择，一些开源库中会有很好的默认选择供我们使用。

2.对选择最大深度的一种直觉是：最大深度越大，则可以构建的决策树就越大，这有点像拟合更高阶的多项式或训练更大的神经网络，它让决策树学习更复杂的模型，但如果将非常复杂的函数拟合到训练数据，则它也会增加过度拟合的风险。

3.理论上，我们可以使用 **交叉验证** 来选择参数，例如**最大深度**，可以在其中尝试不同的最大深度值，然后选择在交叉验证集上效果最好的参数。

尽管在实践中，开源库会有更好的方法来为我们选择最大深度这个参数。