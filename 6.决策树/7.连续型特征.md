# 连续性特征

> 本文描述如何修改决策树以处理不仅具有离散型特征而且具有连续性特征的数据集。



## 1.引入

以下以一个例子进行说明，修改猫分类器的训练数据集，增加一个特征——动物的重量，动物的重量是一个**连续型特征**，而不是仅仅只有几个值的离散型特征：

![image-20240112121437036](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112121437036.png)





### **选用连续型特征进行拆分**

首先，如果我们将根节点的数据集可视化：

![image-20240112134002858](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112134002858.png)

我们可以根据 “动物的重量” 是否小于或等于某个值来拆分数据，例如选择 $阈值=8$，这将数据集划分为两个子集（重量小于等于8，重量大于8）：

![image-20240112134330876](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112134330876.png)

在选用该阈值进行拆分之后，可以计算其带来的信息增益为 $0.24$；

再次尝试其他阈值进行拆分并计算带来的信息增益：

![image-20240112134458341](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112134458341.png)

通过比较选择不同阈值使用该特征进行拆分带来的信息增益，我们可以选择其中一个最好的（即能够带来最大的信息增益）。



在一般情况下，实际上不仅会尝试三个值，还会尝试沿着 `x` 轴的多个值，一种惯例是根据权重或根据此特征的值对所有样本进行排序，并取所有处于已排序训练样本之间的中点的值，这样，如果有10个训练样本，那么我们将测试9个可能阈值，然后尝试选择一个能够提供最高信息增益的值。

最后，如果根据此阈值的给定值进行拆分所获得的信息增益优于根据任何其他特征进行拆分获得的信息增益，那么将决定使用该特征并选用该阈值进行拆分。



总结：让决策树在某个节点上处理使用连续型的特征进行拆分时，只需要考虑选用不同的阈值进行拆分，并执行通常的信息增益计算，并决定在该连续型特征提供尽可能高的信息增益时进行拆分。



尝试不同的阈值，进行通常的信息增益计算，选择能够提供最好的信息增益的阈值对连续值特征进行拆分，并与以其他可能特征进行拆分获得的信息增益比较，如果选用该阈值对该连续性特征进行拆分能够提供最大的信息增益，则选用该连续性特征以该阈值进行拆分。



## 2.课外拓展

在决策树中同时处理离散型特征和连续型特征时，决策树算法会根据不同类型的特征选择最佳的拆分点和划分依据。

- 对于离散型特征，决策树会考虑每个特征的每个可能取值作为划分依据，计算信息增益或其他度量指标（如基尼系数）来评估以每个特征作为划分依据的好坏。然后，选择具有最大信息增益或最小基尼系数的特征作为划分依据。
- 对于连续型特征，决策树需要确定最佳的拆分点（阈值），即在哪一个值处将数据集划分为两个子集。通常采用的方法是遍历所有可能的拆分点，计算信息增益或其他度量指标，并选择最大信息增益或最小基尼系数的拆分点作为最佳的划分点。

具体的拆分方式和度量指标会因决策树算法的实现而有所不同。常见的决策树算法，如`ID3, D3.5 和CART`，都有不同的特征选择策略和度量指标，用于决定最佳的划分特征和划分点。

在实际应用中，决策树算法通常会结合特征选择策略和剪枝技术来选择最佳的划分特征和划分点，以避免过度拟合和提高模型的泛化能力。



##### 对于连续型特征，如何选择具有最大信息增益的拆分点（阈值）

在决策树中选择连续型特征的最佳拆分点通常通过计算信息增益或其他度量指标来进行评估。下面是一种常用的方法来选择具有最大信息增益的拆分点：

1. **排序特征值**：首先，将连续型特征的取值按照从小到大的顺序进行排序；
2. **计算候选拆分点**：在特征值排序的基础上，找到所有可能的拆分点。一种常见的方法是在相邻特征值之间选取中点作为候选拆分点；
3. **计算信息增益**：对于每个候选拆分点，将数据集根据拆分点划分为两个子集，然后计算每个子集的信息增益或其他度量指标。常用的度量指标包括信息增益、基尼系数等。信息增益可以通过计算划分前后的熵或基尼系数之差来衡量；
4. **选择最佳拆分点**：根据信息增益或其他度量指标的计算结果，选择具有最大信息增益的拆分点作为最佳的拆分点；



上述方法是一种简化的描述，实际的决策树算法可能会采用不同的具体实现方式。例如：

- `CART`算法（Classification and Regression Tress）使用基尼系数来选择最佳的拆分点
- `ID3 和 C4.5` 使用信息增益

此外，还可以考虑其他优化策略来提高特征选择的效率，例如二分搜索等。这些策略可以减少搜索的时间复杂度，加快决策树的训练过程。