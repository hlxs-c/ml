# `XGBoost`提升决策树

目前，决策树集成或决策树最常用的方法或实现有一个名为 `XGBoost`的方法，它运行速度快，开源且易于使用，也被非常成功地应用于赢得许多机器学习竞赛以及许多商业应用程序。



## 1.从`袋装决策树`出发到`XGBoost`

以下是 **袋装决策树** 的构建方法：

![image-20240112165732471](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112165732471.png)

在其中添加一处修改：就是**每次通过这个循环（除第一次外），在使用有放回的采样时，我们不是从所有的 $m$ 个训练样本中以等概率（$\frac{1}{m}$）地方式选择样本，而是更有可能（以更高的概率）地从在之前的训练的决策树中表现不佳的错误分类样本中进行选择。**

![image-20240112170723828](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112170723828.png)

我们将查看到目前为止已经训练过的决策树，看看我们还没有做得很好的地方，然后在构建下一个决策树时，我们将更多地关注我们尚未做好的训练样本。

因此，我们不是查看所有的训练样本，而是将更多的注意力集中在尚未表现良好的训练样本子集上并训练获得新的决策树，以使下一个决策树尝试在之前表现较差的子集上做的更好。

这就是 **提升** 背后的原理，结果证明它可以帮助学习算法更快地学习做得更好。



详细来说，我们将查看之前构建的决策树并返回到原始训练集，注意是原始训练集而不是通过替换生成的随机训练集，对原始训练集的所有样本运行之前构建的决策树，查看其对所有训练样本的预测结果，记录预测错误的训练样本。

在下一次构建决策树时，当使用 ”替换采用“ 来生成一个新的随机训练集时，不是等概率地选择训练样本，而是以更高的概率从之前决策树预测错误的训练样本中进行选择，则下一次构建的决策树的注意力将集中在之前决策树表现较差的训练样本上。

并且 **提升**过程将在迭代过程中总共执行 $B$ 次，在每次迭代中（构建编号为 $b$的决策树时），我们将查看 `[1,b-1]`中的所有决策树在哪些方面做的没有那么好，则在构建编号为 $b$ 的决策树时，将有更高的概率选择先前决策树集合表现不佳的样本。



在实现提升的不同方法中，当今使用最广泛的是 `XGBoost`，它代表极端梯度提升，这是一种非常快速和高效的提升树的开源实现；

`XGBoost` 还可以很好地选择默认的分裂标准和何时停止分裂的标准；

`XGBoost`的创新点之一是它还内置了正则化以防止过度拟合；

![image-20240112172752330](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112172752330.png)



> `XGBoost` 并不是使用 **替换采样**，而实际上是为不同的训练样本分配了不同的方法，所以它实际上不需要生成很多随机选择的训练集，这使得它比使用带替换过程的抽样更为有效。
>
> 但就`XGBoost`如何选择要关注的训练样本而言，上述讲述（以更高概率选择之前表现较差的样本）的方法依旧是正确的。





## 2.使用`XGBoost`

![image-20240112173209467](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20240112173209467.png)



- 分类问题：

  - ```python
    from xgboost import XGBClassifier
    
    model = XGBClassifier()
    
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    ```

    

- 回归问题：

  - ```python
    from xgboost import XGBRegressor
    
    model = XGBRegressor()
    
    model.fit(X_train, y_train)
    
    y_pred = mode.predict(X_test)
    ```

    