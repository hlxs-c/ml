# 特征缩放

1. **`z-score` 归一化/标准化**（`Z-score normailzation`）：
   $$
   x_i = \frac{x_i-\mu_i}{\sigma_i}
   $$

   - 其中 $x_i$ 表示第$i$个特征，$\mu_i$ 表示数据集中第$i$ 个特征的取值的平均值，$\sigma_i$ 是数据集中第$i$个特征的所有取值的标准差
   - `z-score` 标准化也叫做 **标准差标准化**，经过处理的数据符合 **均值为0，标准差为1**
   - **特点**：
     - 假设数据是正态分布
     - 将数值范围缩放到 $0$ 附近，数据变为 **均值为0，标准差为1** 的正态分布
     - 不改变原始数据的分布
   - 适用场景：
     - 这种标准化方法适合大多数类型的数据，也是很多工具的默认标准化方法。如果对数据无从下手可以直接使用该标准化；
     - 如果数据存在异常值和较多噪音，可以使用该标准化间接通过中心化比界面异常值和极端值的影响；
     - 需要使用举例来度量相似性的时候：比如 k近邻、`kmeans`聚类、感知机和`SVM`，或者使用`PCA`降维的时候，标准化的表现更好
     - `z-score`方法是一种中心化方法，会改变稀疏数据的结构，不适合用于对稀疏数据做处理。（稀疏数据是指绝大部分的数据都是0，仅有少部分数据为1）。在很多时候，数据集会存在稀疏性特征，表现为**标准差小**，并由很多元素的值为0。最常见的稀疏数据集是用来做协同过滤的数据集，绝大部分的数据都是0，仅有少部分的数据为1。**对系数数据做标准化，不能采用中心化的方式**，否则会破坏稀疏数据的结构。



参考链接（之后需要重新阅读并整理）：

1. [Feature Engineering: Scaling, Normalization and Standardization --- 特征工程：缩放、归一化和标准化 (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/#What_Is_Normalization?)
2. [Understand Data Normalization in Machine Learning | by Zixuan Zhang | Towards Data Science --- 了解机器学习中的数据规范化 |by 张子轩 |迈向数据科学](https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0)
3. [Why, How and When to Scale your Features | by Sudharsan Asaithambi | GreyAtom | Medium --- 为什么、如何以及何时扩展您的功能 |由 Sudharsan Asaithambi |灰色原子 |中等](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)
4. [Python机器学习实战：特征缩放的3个方法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/250704245)
5. [特征预处理——特征缩放 - tonglin0325 - 博客园 (cnblogs.com)](https://www.cnblogs.com/tonglin0325/p/6214808.html)

